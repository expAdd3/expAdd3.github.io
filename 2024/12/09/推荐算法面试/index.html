<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>推荐算法面试 | LXY's Notes</title><meta name="author" content="expAddThree"><meta name="copyright" content="expAddThree"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1 ML与DL基础1.1 机器学习1.1.1 介绍一个最熟悉的机器学习算法 LR（Logistic regression）：逻辑回归是假设数据服从伯努利分布，通过极大似然估计方法，使用梯度下降来求解参数，达到二分类目的的一个模型。我们在考虑把广义线性模型用于分类的时候，需要如何确定逻辑边界，感知机模型用的是阶跃函数，但是阶跃函数不可导，不能作为广义线性模型的联系函数。逻辑回归对数几率函数代替阶跃函">
<meta property="og:type" content="article">
<meta property="og:title" content="推荐算法面试">
<meta property="og:url" content="http://example.com/2024/12/09/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/index.html">
<meta property="og:site_name" content="LXY's Notes">
<meta property="og:description" content="1 ML与DL基础1.1 机器学习1.1.1 介绍一个最熟悉的机器学习算法 LR（Logistic regression）：逻辑回归是假设数据服从伯努利分布，通过极大似然估计方法，使用梯度下降来求解参数，达到二分类目的的一个模型。我们在考虑把广义线性模型用于分类的时候，需要如何确定逻辑边界，感知机模型用的是阶跃函数，但是阶跃函数不可导，不能作为广义线性模型的联系函数。逻辑回归对数几率函数代替阶跃函">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/kanna.jpg">
<meta property="article:published_time" content="2024-12-09T05:07:50.000Z">
<meta property="article:modified_time" content="2024-12-09T07:07:33.609Z">
<meta property="article:author" content="expAddThree">
<meta property="article:tag" content="面试">
<meta property="article:tag" content="搜广推">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/kanna.jpg"><link rel="shortcut icon" href="/img/myIcon.png"><link rel="canonical" href="http://example.com/2024/12/09/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":-1,"unescape":true,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '推荐算法面试',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-09 15:07:33'
}</script><link rel="stylesheet" href="/css/modify.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/background-1.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/kanna.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">LXY's Notes</span></a><a class="nav-page-title" href="/"><span class="site-name">推荐算法面试</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">推荐算法面试</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-09T05:07:50.000Z" title="发表于 2024-12-09 13:07:50">2024-12-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-09T07:07:33.609Z" title="更新于 2024-12-09 15:07:33">2024-12-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%9D%A2%E8%AF%95/">面试</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%9D%A2%E8%AF%95/%E6%90%9C%E5%B9%BF%E6%8E%A8/">搜广推</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">12.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="1-ML与DL基础"><a href="#1-ML与DL基础" class="headerlink" title="1 ML与DL基础"></a>1 ML与DL基础</h1><h2 id="1-1-机器学习"><a href="#1-1-机器学习" class="headerlink" title="1.1 机器学习"></a>1.1 机器学习</h2><h3 id="1-1-1-介绍一个最熟悉的机器学习算法"><a href="#1-1-1-介绍一个最熟悉的机器学习算法" class="headerlink" title="1.1.1 介绍一个最熟悉的机器学习算法"></a>1.1.1 介绍一个最熟悉的机器学习算法</h3><ul>
<li><p>LR（<strong>Logistic regression</strong>）：逻辑回归是假设数据服从<strong>伯努利分布</strong>，通过<strong>极大似然估计方法</strong>，使用<strong>梯度下降</strong>来求解参数，达到二分类目的的一个模型。我们在考虑把广义线性模型用于分类的时候，需要如何确定逻辑边界，感知机模型用的是阶跃函数，但是阶跃函数不可导，不能作为广义线性模型的联系函数。逻辑回归对数几率函数代替阶跃函数。因为对数几率函数是单调可微的一个函数，所以可以作为联系函数。所以逻辑回归本质上还是广义线性模型。</p>
</li>
<li><p>LR的优缺点：</p>
<ul>
<li><p>形式简单，可解释性好；</p>
</li>
<li><p>它直接对分类概率进行建模，不需要知道真实数据的分布，这和生成式模型相区别，避免了假设错误带来的问题；</p>
</li>
<li><p>不仅能够预测出类别，还能够预测出概率，能够用于很多场景，比如ctr排序中；</p>
</li>
<li><p>对数几率函数任意阶数可导，能够很容易优化；</p>
</li>
<li><p>可以获得特征权重，方便我们进行特征筛选；</p>
</li>
<li><p>训练速度快；</p>
</li>
<li><p>它对稀疏特征效果比较好，因为使用的是w1 w2 w3本质上的线性模型，稀疏数据能够筛选出不稀疏的重要特征。</p>
</li>
<li><p>模型表达能力有限；</p>
</li>
<li><p>样本不均衡很难处理；</p>
</li>
<li><p>在非线性可分数据集上性能有限；</p>
</li>
</ul>
</li>
<li><p>LR推导：</p>
</li>
</ul>

<h3 id="1-1-2-决策树怎么建树，基尼系数公式"><a href="#1-1-2-决策树怎么建树，基尼系数公式" class="headerlink" title="1.1.2 决策树怎么建树，基尼系数公式"></a>1.1.2 决策树怎么建树，基尼系数公式</h3><ul>
<li><p>决策树是什么？</p>
<blockquote>
<p>决策树是机器学习算法中的最好入手的算法之一，利用像树一样的图形来辅助决策，其可解释性强、能够转化为规则、计算速度快，应用非常广泛。在实际应用中，决策树常常作为组合算法的基模型，也被作为白盒模型，用来解释黑盒模型预测结果。</p>
</blockquote>
</li>
<li><p>决策树基本原理：建树和剪枝</p>
<p>下图是我们拿到一个西瓜后，判断其是好瓜还是坏瓜的决策思路，可以概括为建树和剪枝两个步骤。建树指的是在树向下生长的时候如何挑选最有解释力度的变量，以及如何选择最优分割点。剪枝指的是避免树长的太过庞大从而引发过拟合等问题，一般有前剪枝和后剪枝。前剪枝主要用于控制树的合成规模，后剪枝用于删除没有意义的分支。</p>
</li>
</ul>

<ul>
<li><p>信息增益与信息增益率</p>
<p>（1）建树原理</p>
</li>
</ul>
<p>在ID3算法中，使用信息增益来挑选最有解释力度的变量。要了解信息增益首先要知道信息熵。对于一个有有限个数的离散变量D，<strong>信息熵的计算公式如下：</strong></p>

<p> — <img src="推荐算法面试/eq.png" alt="m">表示随机变量<img src="推荐算法面试/eq-1733724076029-5.png" alt="D">中的水平个数</p>
<p> — <img src="推荐算法面试/eq-1733724076030-6.png" alt="p_{i}">表示随机变量<img src="推荐算法面试/eq-1733724076029-5.png" alt="D">在<img src="推荐算法面试/eq-1733724076030-7.png" alt="i">水平下的概率。</p>
<p>信息熵衡量的是随机变量的混乱程度，当变量水平较少时，信息熵较小，反之较大。</p>
<p>当引入另一个随机变量A，将原D变量水平分割，此时可以计算在变量A的各水平下随机变量D的信息熵加权，从而得知在引入随机变量A后随机变量D的混乱程度，被称为<strong>条件熵，计算公式如下：</strong></p>

<p>计算随机变量D的信息熵与在引入A变量后的条件熵，用户条件熵减去原信息熵得到的就是信息增益，信息增益衡量的就是在增加变量A后，随机变量D的混乱程度或者纯净程度的变化。当变化越大时，变量A对随机变量D的影响就越大，<strong>信息增益的计算公式如下：</strong></p>

<p>接下来就可以计算目标变量的信息熵，然后分别计算加入各个变量后的条件熵及其信息增益，选取信息增益最大的变量作为第一个进行分裂，这就是ID3算法的建树原理。（ID3算法允许多叉树出现，模型偏好多水平的变量，C4.5模型用信息增益率解决了这个问题CART树采取控制分叉数量，只做<a target="_blank" rel="noopener" href="https://edu.csdn.net/course/detail/40020?utm_source=glcblog&amp;spm=1001.2101.3001.7020">二叉树</a>）</p>
<p>ID3算法的缺点：倾向于选择水平变量较多的变量作为最重要的变量，并且输入变量必须时分类变量。C4.5算法在ID3算法的基础上，将变量筛选的指标由信息增益改为信息增益率，并且加入了连续变量的离散化。<strong>信息增益率的计算公式如下：</strong></p>

<p>sklearn中C4.5算法只支持数值变量，不支持名义变量，所有变量均当作连续变量处理，因此需要事先对名义变量进行WOE转化，将其转为等级变量。WOE转化的计算公式：</p>

<p> — 表示该箱子中好用户占比与坏用户占比的比值的对数，也叫做优势比。</p>
<p>​    （2）gini系数</p>
<p>CART算法使用二叉树将预测区间划分为若干子集，该算法可用来分类或者数值预测，其选用最优分割变量的指标是基于基尼系数的。</p>
<p>gini系数的计算方法如下：对于连续变量，将数据按照升序排列，然后从小到大依次以相邻数值的中间值作为阈值，将样本分成两组，计算两组样本输出变量值的差异，也称为异质性。理想的分组应该尽量使两组输出变量的异质性总和达到最小，即纯度最大。gini系数越小，纯度越大，gini系数的计算公式如下：</p>

<p>在引入分割变量后的基尼系数为：</p>

<p>​    （3）剪枝</p>
<p>剪枝是为了避免决策树完全生长造成的预测精度提升，但是复杂度提高泛化能力变弱的问题。决策树的剪枝根据方向可分为两种，前剪枝和后剪枝。前剪枝是指通过限制条件，控制树的生长；后剪枝是在树充分生长后，通过一定条件将树结点进行收缩达到剪枝的目的。</p>
<p>前剪枝：限制树的最大深度、控制父结点和子结点的最小样本量或者比例等，来限制树的生长。</p>
<p>后剪枝：计算结点中目标变量预测精度或者误差，在建树时将数据分为两部分，一部分用于训练，而另一部分用于验证，先让决策树充分生长，在修剪过程中不断计算测试样本集的误差，当某结点展开后的误差大于不展开时，则收缩该结点。这是C4.5的剪枝思想。CART树则是综合考虑误差和复杂度来进行剪枝。</p>
<ul>
<li><p>决策树建树算法有三种ID3、C4.5、CART，每个算法主要考虑的事情主要有三个问题：</p>
<ol>
<li><p>选什么特征来当条件？</p>
<blockquote>
<p>选什么特征作为最优特征分割：当我们计算完所有特征基尼指数后，选择其中最小所在特征的作为分裂特征；</p>
</blockquote>
</li>
<li><p>条件判断的属性值是什么？</p>
<blockquote>
<p>条件判断的属性值是什么：判断特征属性是否为为此最指数来分裂；</p>
</blockquote>
</li>
<li><p>什么时候停止分裂，达到我们需要的决策？</p>
<blockquote>
<p>什么时候停止分裂，达到我们需要的决策：分裂的最小收益小于我们的划定的阈值，或者树的深度达到我们的阈值。</p>
</blockquote>
</li>
</ol>
</li>
<li><p>CART：</p>
<ul>
<li>CART树采用基尼系数进行最优特征的选择，构造过程中假设有K类，则样本属于第K类的概率为pk，则定义样本分布的<strong>基尼系数</strong>为：</li>
</ul>

<ul>
<li>根据基尼系数定义，可以得到样本集合D的<strong>基尼指数</strong>，其中ck表述样本集合中第k类的子集：</li>
</ul>

<ul>
<li>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。</li>
</ul>

</li>
</ul>
<h3 id="1-1-3-Adaboost拟合目标是什么"><a href="#1-1-3-Adaboost拟合目标是什么" class="headerlink" title="1.1.3 Adaboost拟合目标是什么"></a>1.1.3 Adaboost拟合目标是什么</h3><blockquote>
<p>Adaboost中每训练完一个弱分类器都就会调整权重，上一轮训练中被误分类的点的权重会增加，在本轮训练中，由于权重影响，本轮的弱分类器将更有可能把上一轮的误分类点分对，如果还是没有分对，那么分错的点的权重将继续增加，下一个弱分类器将更加关注这个点，这是adaboost目标。</p>
</blockquote>
<p><strong>AdaBoost</strong>（Adaptive Boosting）是一种集成学习方法，旨在通过组合多个弱学习器（通常是简单的分类器）来提升模型的预测性能。AdaBoost 的核心目标是通过加权调整训练样本的方式，集中注意力于那些被前一个弱分类器分类错误的样本，从而改善模型的性能。</p>
<p>AdaBoost 通过迭代地训练一系列弱分类器，每次训练时都会根据前一轮分类器的错误率调整训练样本的权重。其拟合目标可以从以下几个方面来理解：</p>
<ol>
<li><strong>减少训练误差：</strong><br>AdaBoost 的主要目标是通过加权组合多个弱分类器来减少整体模型的训练误差。每一轮的弱分类器都根据其错误率（即分类错误的样本所占的比例）来更新样本的权重，重点关注之前被误分类的样本。</li>
<li><strong>调整样本权重：</strong><br>AdaBoost 的训练过程通过调整样本的权重来迫使后续分类器更关注那些被前一轮分类器错误分类的样本。这样，模型可以在接下来的迭代中重点关注这些难以分类的样本。</li>
<li><strong>加权组合分类器：</strong><br>每一轮训练的弱分类器都会根据其错误率分配一个权重（通常是通过错误率的指数函数来计算）。最终的强分类器是这些弱分类器的加权投票（对于分类任务）或加权平均（对于回归任务）。这样，较为准确的弱分类器会对最终结果贡献更多。</li>
</ol>
<p><img src="/2024/12/09/推荐算法面试/image-20241209142151540.png" alt="image-20241209142151540" style="zoom:67%;"></p>
<p>最终预测：</p>
<p><img src="/2024/12/09/推荐算法面试/image-20241209142211931.png" alt="image-20241209142211931" style="zoom:67%;"></p>
<p>AdaBoost 的拟合目标就是通过多次迭代训练，优化弱分类器的组合，使得最终模型在训练集上能够更好地拟合数据，特别是能够减少那些难以分类的样本的误差。通过加权组合多个弱分类器，AdaBoost 实现了对复杂模式的更强学习能力。</p>
<h3 id="1-1-4-Adaboost介绍一下，每个基学习器的权重怎么得到的"><a href="#1-1-4-Adaboost介绍一下，每个基学习器的权重怎么得到的" class="headerlink" title="1.1.4 Adaboost介绍一下，每个基学习器的权重怎么得到的"></a>1.1.4 Adaboost介绍一下，每个基学习器的权重怎么得到的</h3><ul>
<li><p>Adaboost采用boosting算法中的一个经典算法。Boosting是集成学习中的一种方法提升是一个逐步优化集成学习器的过程，即每个个体学习器都在弥补集成学习器的欠缺，从而达到整体的优化。Adaboost通过每次降低个体学习器的分类误差，加大效果好的个体学习器的重要性，得到最终的集成学习器，它的损失函数是指数损失函数。</p>
</li>
<li><p>算法流程:</p>
<ul>
<li>初始化训练样本的权值D<sub>0</sub>。则每一个训练样本的权重被初始化为 1 / m，其中 m 为样本的数量。</li>
<li>迭代训练弱分类器。在迭代过程中，需要对样本权重  D<sub>i </sub>进行更新。如果某个样本点已经被准确地分类，那么在训练下一个弱分类器时，就会降低它的权值；相反，如果该个样本点没有被准确地分类，就会提高它的权值。</li>
<li>将各个弱分类器加权平均得到强分类器。误差率 e 低的弱分类器权重<strong>α</strong>较大，误差率 e 高的弱分类器权重<strong>α</strong>较小。</li>
</ul>
</li>
<li><p>数据权重：Adaboost算法中有两种权重，一种是数据的权重，另一种是弱分类器的权重。其中，数据的权重主要用于弱分类器寻找其分类误差最小的决策点，找到之后用这个最小误差计算出该弱分类器的权重（发言权），分类器权重越大说明该弱分类器在最终决策时拥有更大的发言权。如果训练数据保持不变，那么在数据的某个特定维度上单层决策树找到的最佳决策点每一次必然都是一样的，为什么呢？因为单层决策树是把所有可能的决策点都找了一遍然后选择了最好的，如果训练数据不变，那么每次找到的最好的点当然都是同一个点了。</p>
</li>
<li><p>基学习器权重：</p>

<p>其中，k是第k次迭代，e<sub>k</sub>是第k次迭代的误差，在二分类问题中，每个样本被学习器分为为1，分错为0。ak代表了此学习器在最终集成学习器中的重要性，当分类器e<sub>k</sub>越大，ak越小，此学习器话语权越小。ak理论上界是正无穷，有些改进的adaboost会加入softmax思想修正权重。</p>
</li>
</ul>
<h3 id="1-1-5-介绍下GBDT"><a href="#1-1-5-介绍下GBDT" class="headerlink" title="1.1.5 介绍下GBDT"></a>1.1.5 介绍下GBDT</h3><ul>
<li>gbdt 是通过采用加法模型（即<strong>基函数的线性组合</strong>），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法， gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的<strong>残差</strong>基础上进行训练。</li>
<li>GBDT中的树是回归树（不是分类树），默认选择CART回归树，GBDT用来做回归预测，调整后也可以用于分类。</li>
<li>核心思想是<strong>利用损失函数的负梯度在当前模型的值作为残差的近似值</strong>，本质上是<strong>对损失函数进行一阶泰勒展开</strong>，从而拟合一个回归树。</li>
</ul>
<p><strong>Gradient Boosting Decision Trees (GBDT)</strong>，即<strong>梯度提升决策树</strong>，是一种强大的集成学习方法，广泛应用于分类、回归等任务。它通过迭代地训练多个决策树（通常是浅层决策树），并通过逐步优化损失函数来提升模型的预测性能。GBDT 是基于梯度提升（Gradient Boosting）框架的一种实现，具有高效、灵活、精确的特点。</p>
<p><strong>GBDT 的工作原理</strong></p>
<p>GBDT 的核心思想是通过逐步拟合目标函数的负梯度方向（即最小化损失函数），不断提高模型的性能。其主要步骤如下：</p>
<ol>
<li><strong>初始化模型</strong>： GBDT 的初始模型通常是一个常数值，这个常数是通过最小化损失函数得到的（比如回归问题中可以取目标变量的均值，分类问题中可以取类别的均值或最大概率类别）。</li>
<li><strong>迭代训练：</strong> GBDT 通过一系列迭代过程，逐步改进模型：<ul>
<li>在每一轮迭代中，模型计算当前模型预测值与真实值之间的残差（即误差）。</li>
<li>接着，使用一个新的决策树来拟合这个残差，通常选择一个决策树作为弱学习器来捕捉残差的模式。</li>
<li>新的树的预测值是用来更新现有模型的，更新的方向是当前模型残差的负梯度。</li>
</ul>
</li>
<li><strong>更新模型</strong>： 在每一轮训练中，GBDT 对当前模型进行更新，更新的方式是通过加上新训练出的树的预测值，更新模型的权重。每棵树的贡献有一个学习率参数来控制。</li>
<li><strong>学习率（Shrinkage）</strong>： GBDT 通常会引入一个学习率（也叫缩减因子）来控制每次迭代中新增决策树的贡献。学习率通常小于 1，以避免过拟合，并提高模型的泛化能力。最终的预测是所有树的加权和。</li>
<li><strong>终止条件</strong>： GBDT 在迭代过程中通常会设定终止条件，常见的条件有：<ul>
<li>达到最大迭代次数 TTT</li>
<li>模型在验证集上的误差停止改进（早停）</li>
</ul>
</li>
</ol>
<p><strong>主要步骤总结：</strong></p>
<ul>
<li>初始化：用常数初始化模型（比如目标值的均值或类别的最大概率）。</li>
<li>迭代：<ul>
<li>在每次迭代中，计算当前模型的残差。</li>
<li>构建一个新的决策树来拟合这些残差。</li>
<li>更新模型，使其包含新的树。</li>
<li>调整学习率，控制新树的影响力。</li>
</ul>
</li>
<li><strong>终止</strong>：当达到最大迭代次数或模型性能不再改进时停止。</li>
</ul>
<p><strong>数学原理：</strong></p>
<p><img src="/2024/12/09/推荐算法面试/image-20241209142839020.png" alt="image-20241209142839020" style="zoom:67%;"></p>
<p><strong>GBDT 的优点</strong></p>
<ol>
<li><strong>高效性和精确性</strong>： GBDT 能够在很多机器学习任务中取得很好的性能，尤其是在处理复杂的非线性关系时，比传统的线性模型具有更强的表达能力。</li>
<li><strong>灵活性</strong>： GBDT 可以与各种损失函数配合使用，因此能够解决分类、回归、排序等多种任务。</li>
<li><strong>抗过拟合能力</strong>： 由于引入了学习率和树的深度限制，GBDT 相比于其他集成方法（如随机森林）能够更好地控制模型的复杂度，减少过拟合。</li>
<li><strong>处理缺失值和类别特征</strong>： GBDT 可以自然地处理缺失值和类别特征，而无需过多的数据预处理。</li>
</ol>
<p><strong>GBDT 的缺点</strong></p>
<ol>
<li><strong>训练时间较长</strong>： GBDT 的训练时间相对较长，尤其是树的数量和深度增加时，训练速度会受到影响。</li>
<li><strong>对噪声敏感</strong>： 如果数据中存在很多噪声，GBDT 可能会过拟合，尤其是在迭代次数较多时。</li>
<li><strong>参数调节复杂</strong>： GBDT 有多个超参数（如树的深度、学习率、树的数量等），需要通过交叉验证等方法进行调优。</li>
</ol>
<p><strong>常见的 GBDT 实现</strong></p>
<ul>
<li><strong>XGBoost</strong>（Extreme Gradient Boosting）：目前最流行的 GBDT 实现之一，提供了高效的计算和优化，支持正则化，能够有效防止过拟合。</li>
<li><strong>LightGBM</strong>（Light Gradient Boosting Machine）：由 Microsoft 提供的一个 GBDT 实现，专注于大规模数据的高效训练。</li>
<li><strong>CatBoost</strong>：由 Yandex 提供，特别擅长处理类别特征，并且具有较好的默认参数设置。</li>
</ul>
<p>GBDT 是一种强大的提升方法，通过逐步训练决策树来减少模型的误差。它通过梯度下降的方式优化损失函数，集成多个弱分类器（决策树）来实现强大的预测能力。由于其灵活性和高效性，GBDT 在许多机器学习竞赛中都取得了优异的成绩，如 Kaggle 等平台。</p>
<h4 id="1-1-5-1-为什么GBDT中使用回归树而不是分类树？"><a href="#1-1-5-1-为什么GBDT中使用回归树而不是分类树？" class="headerlink" title="1.1.5.1 为什么GBDT中使用回归树而不是分类树？"></a>1.1.5.1 为什么GBDT中使用回归树而不是分类树？</h4><p>在 <strong>GBDT (Gradient Boosting Decision Trees)</strong> 中，所使用的树通常是 <strong>回归树 (Regression Trees)</strong>，而不是分类树。这是因为 GBDT 是通过拟合模型的 <strong>残差</strong>（即预测误差）来逐步改进预测，因此每棵树的输出是一个连续值，而非类别标签。</p>
<p><strong>为什么使用回归树而非分类树？</strong></p>
<ol>
<li><strong>目标是拟合残差</strong>：<br>GBDT 的核心是通过逐步拟合目标函数的负梯度来最小化损失函数。在回归问题中，这个负梯度通常是一个连续的实数值，代表了预测值与实际值之间的差异。因此，使用回归树来拟合这些残差是自然的选择。回归树的输出是一个连续值，而分类树的输出是一个类别标签。</li>
<li><strong>树的输出是残差</strong>：<br>在 GBDT 中，每一轮的弱分类器（即树）不是直接进行分类或回归预测，而是拟合上一步迭代中的 <strong>残差</strong>（即当前模型的预测误差）。这些残差通常是连续的，因此我们需要使用 <strong>回归树</strong> 来进行拟合。</li>
<li><strong>树的构建方法</strong>：<ul>
<li><strong>回归树</strong>：回归树在每个节点的分裂是基于最小化平方误差（MSE）来决定的，即通过最小化每个子节点中样本的均方差来选择最佳的分裂特征和分裂点。</li>
<li><strong>分类树</strong>：分类树则通常使用基于信息增益（例如熵或基尼不纯度）的标准来进行节点分裂，输出的是一个类别标签。</li>
</ul>
</li>
</ol>
<p><strong>具体情况：</strong></p>
<ul>
<li><strong>回归任务</strong>：在回归任务中，GBDT 使用回归树来拟合每一轮的残差。每棵回归树的输出是一个连续的数值，表示模型预测误差的修正值。每次迭代通过添加新的树来修正当前模型的残差，逐步优化模型的拟合效果。</li>
<li><strong>分类任务</strong>：尽管目标是分类任务，但 GBDT 中的每棵树依然是回归树。在分类任务中，树的输出依然是一个连续值（通常是某种 <strong>得分</strong> 或 <strong>对数几率</strong>），而不是直接的类别标签。最终的分类决策是通过将所有树的输出加权求和，得到一个类别的得分，再通过一个阈值（例如使用 <strong>sigmoid</strong> 函数将得分转化为概率，或者通过投票方式决定类别）来确定最终类别。</li>
</ul>
<p><strong>例子：</strong></p>
<ul>
<li><strong>回归问题</strong>：假设目标是预测房价，GBDT 会通过多次迭代，每次迭代用回归树拟合前一次迭代的残差（房价的预测误差），每棵树的输出是连续的修正值（例如，提升或减少房价预测的部分）。</li>
<li><strong>分类问题</strong>：假设目标是二分类问题（如垃圾邮件分类），每棵树依然是回归树，输出的是一个连续值，通常表示样本属于某一类别的 <strong>对数几率</strong>。最终的分类决策是通过将这些连续的输出值加权求和后使用 sigmoid 函数将其转化为概率，再根据阈值决定类别。</li>
</ul>
<p><strong>总结：</strong></p>
<p>在 GBDT 中，无论是回归任务还是分类任务，所使用的树通常是 <strong>回归树</strong>。即使是分类任务，每棵树的输出也是一个连续值（如概率或得分），最终通过模型输出的加权和来进行决策。因此，GBDT 中的树本质上是回归树，负责拟合残差并不断优化模型的性能。</p>
<h3 id="1-1-6-介绍XGBoost"><a href="#1-1-6-介绍XGBoost" class="headerlink" title="1.1.6 介绍XGBoost"></a>1.1.6 介绍XGBoost</h3><ul>
<li>XGBoost是的陈天奇开源一种梯度提升树模型，是GBDT的一种工程实现。与GBDT最大的区别就是<strong>树的生成方式不同</strong>，加快了树的生成过程，以生成最优树。</li>
<li>XGBT相对于GBDT的优化（sklearn中的GBDT实现和传统的有一定改进，同样支持XGBT的一些特性，这里的对比只针对传统GBDT）：<ul>
<li><strong>正则项</strong>：XGBT加入了正则项，控制模型复杂度，防止过拟合，加入的有叶子结点个数正则化、叶子结点输出L2正则化；</li>
<li><strong>二阶泰勒展开</strong>：XGBT采用牛顿法，对损失函数进行了二阶泰勒展开，加速收敛速度；</li>
<li><strong>支持更多基学习器</strong>：GBDT只支持CART树，XGBT支持多种基学习器，比如线性分类器；</li>
<li><strong>行采样</strong>：传统GBDT每一轮迭代都使用了全部数据，XGBT使用了行采样；</li>
<li><strong>列采样</strong>：传统GBDT同样没有使用列采样，XGBT引入了列采样；</li>
<li><strong>缺失值处理</strong>：GBDT没有缺失值处理机制，XGBT支持缺失值处理；</li>
<li><strong>Shrinkage</strong>：对每一颗树输出进行衰减，削弱单颗树影响，让后续树有更大学习空间；</li>
<li><strong>并行化</strong>：特征粒度的并行化，而非树粒度的，在不同特征上采用多线程并行计算最优分割点；</li>
</ul>
</li>
</ul>
<h3 id="1-1-7-介绍下LightGBM"><a href="#1-1-7-介绍下LightGBM" class="headerlink" title="1.1.7 介绍下LightGBM"></a>1.1.7 介绍下LightGBM</h3><blockquote>
<p>LightGBM是微软开源的一个梯度Boosting框架，使用基于决策树的学习算法，是GBDT的一种工程实现，特点是<strong>快</strong>。</p>
</blockquote>
<p><strong>LightGBM</strong>（Light Gradient Boosting Machine）是由微软（Microsoft）提出的一个高效的梯度提升决策树（GBDT）实现，专为大规模数据和高效训练设计。它在速度、内存使用和准确性方面都进行了优化，成为了机器学习竞赛中常用的工具之一，尤其适用于大数据集和高维度问题。</p>
<h4 id="1-1-7-1-LightGBM核心特点"><a href="#1-1-7-1-LightGBM核心特点" class="headerlink" title="1.1.7.1 LightGBM核心特点"></a>1.1.7.1 LightGBM核心特点</h4><ol>
<li><strong>高效的训练速度</strong>：<ul>
<li>LightGBM 采用了许多优化技术，使得训练速度比传统的 GBDT 实现（如 XGBoost）更快。它特别适合于大规模数据集，能够有效处理数百万行的数据。</li>
</ul>
</li>
<li><strong>支持类别特征</strong>：<ul>
<li>LightGBM 能够直接处理 <strong>类别特征</strong>，不需要手动进行独热编码（One-Hot Encoding）。这种支持极大地减少了数据预处理的复杂性，同时减少了计算的内存占用和训练时间。</li>
</ul>
</li>
<li><strong>叶子-wise（Leaf-wise）树生长策略</strong>：<ul>
<li>LightGBM 采用 <strong>叶子-wise</strong> 的树生长方式，而传统的 GBDT（如 XGBoost）使用 <strong>层级-wise</strong> 的生长方式。在叶子-wise 方法中，树的生长优先选择使损失函数减少最多的叶子节点进行分裂，这通常可以提高模型的准确性。</li>
<li>这种生长策略虽然比层级-wise 方法在某些情况下更容易过拟合，但通过正则化技术（如深度限制、叶子数限制和学习率）可以很好地控制。</li>
</ul>
</li>
<li><strong>直方图算法</strong>：<ul>
<li>LightGBM 使用 <strong>直方图算法</strong> 来加速数据分裂的计算。通过将连续特征离散化为固定的区间（binning），LightGBM 能够显著减少内存消耗和计算开销，同时提高训练速度。</li>
<li>每个特征值会被映射到一个整数区间，然后通过计算这些区间的梯度和统计量来进行分裂决策。这种方法能够有效减少数据存储量，并加快特征的分裂过程。</li>
</ul>
</li>
<li><strong>支持并行和分布式训练</strong>：<ul>
<li>LightGBM 可以在多核CPU上进行 <strong>数据并行</strong> 和 <strong>特征并行</strong>，能够充分利用硬件的并行计算能力。此外，它也支持分布式训练，可以在多个机器之间分配计算任务，适合大规模数据集的训练。</li>
</ul>
</li>
<li><strong>内存优化</strong>：<ul>
<li>LightGBM 在内存使用方面进行了优化，特别是在训练大规模数据时，比其他 GBDT 实现（如 XGBoost）更节省内存，能够处理更大的数据集。</li>
</ul>
</li>
<li><strong>支持增量学习</strong>：<ul>
<li>LightGBM 支持 <strong>增量学习</strong>（也叫<strong>在线学习</strong>），可以在现有模型的基础上继续训练，这对处理大规模数据流或者不断更新的数据集非常有用。</li>
</ul>
</li>
</ol>
<h4 id="1-1-7-2-LightGBM主要技术创新"><a href="#1-1-7-2-LightGBM主要技术创新" class="headerlink" title="1.1.7.2 LightGBM主要技术创新"></a>1.1.7.2 LightGBM主要技术创新</h4><ol>
<li><strong>直方图分裂（Histogram-based Splitting）</strong>：<ul>
<li>传统的 GBDT 在每次分裂时要遍历每个特征的所有数据点，计算每个可能的分裂点。LightGBM 通过将数据划分为多个区间（bin），减少了可能的分裂点数量，从而提高了计算效率。</li>
</ul>
</li>
<li><strong>叶子-wise 树增长策略</strong>：<ul>
<li>传统的 GBDT 使用 <strong>层级-wise</strong> 的树生长策略，即每次分裂都会尝试分裂每一层的所有节点。而 LightGBM 采用 <strong>叶子-wise</strong> 的策略，优先分裂损失下降最多的叶子节点，从而更加高效地利用每一次的分裂。</li>
</ul>
</li>
<li><strong>基于梯度的单边采样（Gradient-based One-Side Sampling, GOSS）</strong>：<ul>
<li>在每次迭代时，LightGBM 对于梯度较小的样本采用 <strong>单边采样</strong>（GOSS）。梯度较小的样本通常对损失函数的优化贡献较小，因此可以通过采样来减少计算量，而保持模型的准确性。</li>
</ul>
</li>
<li><strong>数据并行和特征并行</strong>：<ul>
<li>在分布式训练中，LightGBM 使用 <strong>数据并行</strong> 和 <strong>特征并行</strong>，分别将数据和特征分配给不同的工作节点进行并行计算。这种方法显著提高了大规模数据集的训练效率。</li>
</ul>
</li>
</ol>
<h4 id="1-1-7-LightGBM相对于XGBoost的改进"><a href="#1-1-7-LightGBM相对于XGBoost的改进" class="headerlink" title="1.1.7. LightGBM相对于XGBoost的改进"></a>1.1.7. LightGBM相对于XGBoost的改进</h4><ul>
<li>LGB相对于XGBT的改进：<ul>
<li>基于<strong>直方图的决策树算法</strong>：把特征离散到K个bin中构造直方图，遍历一遍特征统计直方图，最后根据直方图寻找最优分割点，这样做的好处是计算速度更快，内存占用更小；</li>
<li><strong>直方图做差加速</strong>：计算兄弟节点的直方图，只需要用父节点直方图-本节点直方图。这样做速度提升了一倍；</li>
<li><strong>Leaf-wise叶子生长策略</strong>：XGBT的Level-wise每次分裂一层节点，容易并行化，但是更容易过拟合，Leaf-wise值分裂增益最大的节点，相对精度更高，过拟合更小；</li>
<li>直接<strong>支持类别特征</strong>：第一个直接支持类别特征的GBDT工具，具体算法《On Grouping For Maximum Homogeneity》；</li>
<li><strong>高效并行优化</strong>：数据量小采用特征并行、数据并行，数据量大采用投票并行；</li>
<li><strong>Cache优化</strong>：直方图算法天生提高缓存命中，降低内存消耗；</li>
<li><strong>单边梯度抽样算法</strong>：过滤梯度小的样本，同时平衡了数据分布的改变，这个算法能够提升计算速度；</li>
</ul>
</li>
</ul>

<h3 id="1-1-8-GBDT中的梯度是什么，怎么用"><a href="#1-1-8-GBDT中的梯度是什么，怎么用" class="headerlink" title="1.1.8 GBDT中的梯度是什么，怎么用"></a>1.1.8 GBDT中的梯度是什么，怎么用</h3><p>在线性模型优化的过程中。利用<strong>梯度下降</strong>我们总是让参数<strong>向负梯度的方向移动</strong>，使损失函数最小。gbdt，假入我们现在有 t 课树，我们需要去学习是第 t+1 颗树，那么如何学习第 t+1 颗树才是最优的树呢？ 这个时候我们参考梯度优化的思想。现在的 t 课树就是我们现在的状态使用这个状态我们可以计算出现在的损失。如何让损失更小呢？我们只需要让 t+1 颗树去拟合损失的负梯度。而<strong>残差</strong> 是梯度在MSE为损失函数下的特例（MSE的导数就是残差）。</p>
<h3 id="1-1-9-GBDT如何计算特征重要性"><a href="#1-1-9-GBDT如何计算特征重要性" class="headerlink" title="1.1.9 GBDT如何计算特征重要性"></a>1.1.9 GBDT如何计算特征重要性</h3><ul>
<li>所有集成树可以通过单颗树的特征重要度平均来计算；</li>
<li>单颗树的特征重要度（用CART树举例）有几种算法（重点要和面试官介绍处第一种）：<ul>
<li>可以通过基尼系数特征分裂收益计算重要性：训练过程中通过记录特征的分裂总次数、总/平均信息增益来对特征重要性进行量化。例如实际工程中我们会用特征在整个GBDT、XgBoost里面被使用的次数或者带来的总/平均信息增益来给特征重要度打分，最后进行排序。由于本身Ensemble模型在选择特征分裂时带有一定随机性，一般会跑多个模型然后把特征重要性求平均后排序。</li>
<li>通过OOB数据来计算：训练好模型之后，用Out of Bag（或称Test）数据进行特征重要性的量化计算。具体来说，先用训练好的模型对OOB数据进行打分，计算出AUC或其他业务定义的评估指标；接着对OOB数据中的每个特征： （1）随机shuffle当前特征的取值； （2）重新对当前数据进行打分，计算评估指标； （3）计算指标变化率</li>
</ul>
</li>
</ul>
<h3 id="1-1-10-GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差"><a href="#1-1-10-GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差" class="headerlink" title="1.1.10 GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差"></a>1.1.10 GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差</h3><ul>
<li>核心思想是<strong>利用损失函数的负梯度在当前模型的值作为残差的近似值</strong>，本质上是<strong>对损失函数进行一阶泰勒展开</strong>，从而拟合一个回归树。</li>
<li>而<strong>残差</strong> 是梯度在MSE为损失函数下的特例（MSE的导数就是残差）。</li>
</ul>
<p>GPT的回答：</p>
<p>在 <strong>GBDT (Gradient Boosting Decision Trees)</strong> 中，每一轮的模型拟合目标是<strong>残差</strong>，而不是直接拟合真实的误差。这里的“残差”实际上是模型在当前阶段的 <strong>预测误差</strong>（即模型的当前预测值与真实标签之间的差异）。然而，在某些情况下，这个残差确实接近“真实的误差”，但它并不总是严格的真实误差，具体取决于我们如何定义和理解误差的来源。</p>
<p>残差能看作是“真实误差”的情况通常是在以下条件下成立：</p>
<ul>
<li><strong>模型的偏差很小（无偏估计）</strong>：如果模型非常准确，且具有很好的拟合能力，那么模型的预测值接近真实值，这时计算出的残差会接近真实误差。</li>
<li><strong>没有系统性误差或噪声</strong>：如果数据没有噪声，或者噪声非常小，模型能够很好地拟合数据，残差的分布会接近零。这种情况下，残差就接近于真实的误差。</li>
<li><strong>模型的误差趋于随机性</strong>：如果当前模型已经很难进一步优化，所有的偏差都已消除，那么误差（残差）将会是随机的，接近一个零均值的噪声，残差此时可以视为真实误差的近似。</li>
</ul>
<h3 id="1-1-11-介绍XGBoost中的并行"><a href="#1-1-11-介绍XGBoost中的并行" class="headerlink" title="1.1.11 介绍XGBoost中的并行"></a>1.1.11 介绍XGBoost中的并行</h3><p>这里可以参考前面那句话：数据量小采用特征并行、数据并行，数据量大采用投票并行；</p>
<p>（1）<strong>XGBoost 的并行计算框架</strong></p>
<p>XGBoost 通过以下几种方式进行并行化处理：</p>
<ul>
<li><strong>特征并行（Feature Parallelism）</strong>：多个特征（或者说多个数据列）可以同时计算。</li>
<li><strong>数据并行（Data Parallelism）</strong>：数据样本可以在多个线程上并行处理。</li>
<li><strong>梯度和分裂并行（Split Parallelism）</strong>：在决策树构建过程中，多个分裂点的计算可以并行执行。</li>
</ul>
<p>（2）<strong>XGBoost 的并行化类型</strong></p>
<ol>
<li><strong>树构建过程中的并行化</strong></li>
</ol>
<p>XGBoost 中的树构建过程使用了两种主要的并行化技术：<strong>按列（特征）并行</strong>和<strong>按行（样本）并行</strong>。</p>
<ul>
<li><strong>按列并行（Feature Parallelism）</strong>：<ul>
<li>在每一次 <strong>节点分裂</strong> 时，XGBoost 会遍历所有特征来寻找最佳的分裂点。为了加速这个过程，XGBoost 将每个特征的分裂计算任务分配给不同的线程进行并行计算。</li>
<li>在每个节点上，XGBoost 会计算所有特征的最佳分裂点（特征的增益、最优分裂点等），每个特征的计算可以在不同的 CPU 核心上并行处理。这样可以有效减少节点分裂的计算时间。</li>
</ul>
</li>
<li><strong>按行并行（Data Parallelism）</strong>：<ul>
<li>数据并行是指对不同样本的计算进行并行化。在 XGBoost 的训练过程中，每一轮分裂都会计算每个样本在不同特征下的增益。这些样本的计算可以分配到不同的线程上并行计算。</li>
<li>在每个节点的分裂中，XGBoost 对每个特征在所有样本中的增益进行计算（计算所有特征的梯度和二阶梯度）。通过将这些样本分配给多个线程并行计算，可以加速这一过程。</li>
</ul>
</li>
</ul>
<ol>
<li><strong>分裂点的并行计算</strong></li>
</ol>
<p>在构建决策树的过程中，XGBoost 会为每一个节点选择最优的分裂点。每个特征的分裂点计算是独立的，可以并行处理。</p>
<ul>
<li>XGBoost 会计算每个特征的增益（即该特征在某个分裂点处的损失函数减少量）。</li>
<li>由于每个特征的计算是独立的，因此这些计算可以在多个线程之间并行执行。</li>
</ul>
<ol>
<li><strong>使用直方图方法加速</strong></li>
</ol>
<p>XGBoost 提供了直方图算法（Histogram-based algorithm），使得计算更高效。通过将连续特征值离散化为多个区间（bins），XGBoost 可以减少计算量，从而加速训练。</p>
<ul>
<li><strong>直方图方法</strong>：在每一轮训练中，XGBoost 会先将连续特征的值映射到固定的区间中（即进行离散化），然后通过计算这些区间的增益来寻找最优分裂点。这种方法使得特征的增益计算变得更加高效，特别是在数据量大时。</li>
</ul>
<p>直方图方法不仅可以加速计算，还可以通过并行化进一步提高效率。每个区间的增益计算可以并行进行，从而加速训练过程。</p>
<ol>
<li><strong>多线程计算与 CPU 优化</strong></li>
</ol>
<p>XGBoost 可以通过多线程（利用多核 CPU）来加速计算。XGBoost 默认使用所有可用的 CPU 核心来并行计算树的分裂过程，并支持以下两种并行策略：</p>
<ul>
<li><strong>Multi-threading</strong>：通过将任务分配到多个线程，XGBoost 能够充分利用现代多核 CPU 的计算能力，显著提高计算速度。</li>
<li><strong>Block-based Parallelism</strong>：XGBoost 在进行树构建时，首先会将样本划分为多个“块”，然后并行处理每个块。每个块中的样本会在多个线程上并行计算，进一步提高计算效率。</li>
</ul>
<ol>
<li><strong>GPU 加速（可选）</strong></li>
</ol>
<p>XGBoost 还支持 <strong>GPU 加速</strong>，这对于大规模数据集尤其有效。通过将计算任务转移到 GPU 上，XGBoost 能够利用 GPU 强大的并行处理能力，进一步提高训练速度。</p>
<ul>
<li><strong>GPU 并行</strong>：XGBoost 支持在 GPU 上计算树的分裂点，计算增益等操作，这比在 CPU 上计算要快得多。</li>
<li><strong>适用场景</strong>：对于非常大的数据集，或者在需要大量迭代的模型（如深度树模型）中，GPU 加速可以显著减少训练时间。</li>
</ul>
<h3 id="1-1-12-介绍XGBoost中精确算法与近似算法"><a href="#1-1-12-介绍XGBoost中精确算法与近似算法" class="headerlink" title="1.1.12 介绍XGBoost中精确算法与近似算法"></a>1.1.12 介绍XGBoost中精确算法与近似算法</h3><ul>
<li>指的是在计算特征分裂的时候，XGBT使用了近似算法</li>
<li>精确算法：通过列举所有特征的可能划分找到最优划分解来生成树，该方法<strong>需要排序以形成连续的特征，之后计算每个可能的梯度统计值</strong>。<ul>
<li>缺点：在数据量非常大的情况下，精确基本用不了。一方面在生成树的过程中，每次寻找一个节点最佳分割点时，都需要比较其在所有特征的<strong>所有分割点</strong>上的效果，这么做时间复杂度很高；另一方面，在每次对某个特征进行分割的时候，需要对所有样本根据该特征值进行排序，这需要我们把所有的数据存储在内存中，这会给硬件方面带来很大压力。</li>
</ul>
</li>
<li>近似算法：在针对一个特征寻找分割点的时候，我们其实对特征中的值的范围不敏感，只对这些值的顺序敏感。比如数据集中的样本的某一个特征出现的值有12、15、82、107，但是如果我们把这四种值分别替换成1、2、3、4，最后得到的树的结构是不变的。利用这种思想，给出一个数据集中样本的第k个特征和样本点在损失函数上的二阶导数所组成的集合，随后利用数据分布的百分比来定义一个排名函数 ，这个排名函数代表了特征k的值小于z的样本占总样本的比例。我们的目标就是利用这个排名函数来寻找候选分割点集合。</li>
</ul>
<h3 id="1-1-13-XGBoost如何处理空缺值，为何要进行行采样、列采样"><a href="#1-1-13-XGBoost如何处理空缺值，为何要进行行采样、列采样" class="headerlink" title="1.1.13 XGBoost如何处理空缺值，为何要进行行采样、列采样"></a>1.1.13 XGBoost如何处理空缺值，为何要进行行采样、列采样</h3><ul>
<li>将缺失值分别划分到左子树和右子树，分别计算出左子树和右子树的增益 ，选出更大的，将该方向作为缺失值的分裂方向（记录下来，预测阶段将会使用）；LGB使用相同的方法；</li>
<li>降低了过拟合；</li>
</ul>
<h3 id="1-1-14-讲一下xgboost算法，xgboost是如何处理离散特征的，xgb怎么训练，xgb算法优点，怎么选特征，主要参数有哪些，xgb的特征重要性怎么看"><a href="#1-1-14-讲一下xgboost算法，xgboost是如何处理离散特征的，xgb怎么训练，xgb算法优点，怎么选特征，主要参数有哪些，xgb的特征重要性怎么看" class="headerlink" title="1.1.14 讲一下xgboost算法，xgboost是如何处理离散特征的，xgb怎么训练，xgb算法优点，怎么选特征，主要参数有哪些，xgb的特征重要性怎么看"></a>1.1.14 讲一下xgboost算法，xgboost是如何处理离散特征的，xgb怎么训练，xgb算法优点，怎么选特征，主要参数有哪些，xgb的特征重要性怎么看</h3><h3 id="1-1-15-xgboost介绍一下，xgb对目标函数二阶泰勒展开，哪个是x，哪个是delta-x-一阶导和二阶导是对谁求得"><a href="#1-1-15-xgboost介绍一下，xgb对目标函数二阶泰勒展开，哪个是x，哪个是delta-x-一阶导和二阶导是对谁求得" class="headerlink" title="1.1.15 xgboost介绍一下，xgb对目标函数二阶泰勒展开，哪个是x，哪个是delta x, 一阶导和二阶导是对谁求得"></a>1.1.15 xgboost介绍一下，xgb对目标函数二阶泰勒展开，哪个是x，哪个是delta x, 一阶导和二阶导是对谁求得</h3><h3 id="1-1-16-为什么高维稀疏数据，LR比GBDT要好"><a href="#1-1-16-为什么高维稀疏数据，LR比GBDT要好" class="headerlink" title="1.1.16 为什么高维稀疏数据，LR比GBDT要好"></a>1.1.16 为什么高维稀疏数据，LR比GBDT要好</h3><ul>
<li>树模型对稀疏特征，切分的收益非常小，只能从少量非0信息上学习；</li>
<li>线性模型的正则项是对权重惩罚，树模型是对深度、叶子个数的惩罚。所以高维稀疏数据中，少量样本会对结果产生非常大的影响，非常容易过拟合，而线性模型的权重惩罚能够很好处理这一点。综上，带正则化的线性模型比较不容易对稀疏特征过拟合；</li>
<li>同样的原因可以解释为什么onehot不适合树模型；</li>
</ul>
<h3 id="1-1-17-随机森林与GBDT采样的区别"><a href="#1-1-17-随机森林与GBDT采样的区别" class="headerlink" title="1.1.17 随机森林与GBDT采样的区别"></a>1.1.17 随机森林与GBDT采样的区别</h3><ul>
<li>RF采用了行列采样，传统GBDT算法没有采用（SkLearn中集成了采样）；</li>
</ul>
<h3 id="1-1-18-随机森林中列采样的作用"><a href="#1-1-18-随机森林中列采样的作用" class="headerlink" title="1.1.18 随机森林中列采样的作用"></a>1.1.18 随机森林中列采样的作用</h3><ul>
<li>随机森林在bagging基础上，进一步在训练过程引入随机属性选择，从全集d中随机选择k个属性的子集，利用这个子集来建立本颗子树，下一轮同理；推荐的k=log2d；</li>
<li>能够有效降低过拟合风险，降低方差；</li>
</ul>
<h3 id="1-1-19-bagging与boosting对比-boosting和bagging的区别及分别适用于什么场景"><a href="#1-1-19-bagging与boosting对比-boosting和bagging的区别及分别适用于什么场景" class="headerlink" title="1.1.19 bagging与boosting对比, boosting和bagging的区别及分别适用于什么场景"></a>1.1.19 bagging与boosting对比, boosting和bagging的区别及分别适用于什么场景</h3><ul>
<li>boosting：串行的方式训练基分类器，各分类器之间有依赖。每次训练时，对前一层基分类器分错的样本给与更高的权重，更多的关注的是偏差；</li>
<li>bagging：是Bootstrap aggregating的意思，各分类器之间无强依赖，可以并行，最终结果进行投票（分类），或者平均（回归）；</li>
<li>样本选择上：<ul>
<li>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。</li>
<li>Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。</li>
</ul>
</li>
<li>样例权重：<ul>
<li>Bagging：使用均匀取样，每个样例的权重相等。</li>
<li>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。</li>
</ul>
</li>
<li>预测函数：<ul>
<li>Bagging：所有预测函数的权重相等。</li>
<li>Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。</li>
</ul>
</li>
<li>并行计算：<ul>
<li>Bagging：各个预测函数可以并行生成。</li>
<li>Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。</li>
</ul>
</li>
</ul>
<h3 id="1-1-20-bagging与boosting分别从什么角度降低过拟合"><a href="#1-1-20-bagging与boosting分别从什么角度降低过拟合" class="headerlink" title="1.1.20 bagging与boosting分别从什么角度降低过拟合"></a>1.1.20 bagging与boosting分别从什么角度降低过拟合</h3><ul>
<li>bagging降低方差，boosting降低偏差</li>
</ul>
<ul>
<li>逻辑回归如何避免过拟合</li>
<li>推导逻辑回归损失函数和损失函数求导</li>
<li>正则化项L1和L2为什么有用</li>
<li>l1正则不可导，如何优化</li>
<li>什么样的特征容易产生比较小的权重</li>
<li>随机森林采样n次，n趋于无穷大，oob样本的概率接近于？</li>
<li>逻辑回归与树模型的优缺点</li>
<li>对于高维稀疏数据，树模型能训练吗？一般怎么处理</li>
<li>树模型一般有哪些参数，分别有什么作用</li>
<li>随机森林如何处理空缺值</li>
<li>介绍kmeans，与其他聚类算法的对比</li>
<li>机器学习导致误差的原因？过拟合、欠拟合对应的偏差和方差是怎样的？</li>
<li>如何解决过拟合问题？哪些角度</li>
<li>LR的原理，问题的假设，为什么用交叉熵作为损失函数</li>
<li>LR损失函数写一下</li>
<li>LR是不是凸优化问题，如何判断LR达到最优值</li>
<li>LR一般用什么数据，一般有什么特点(离散数据，离散化的一堆优点)</li>
<li>LR，SVM, xgboost如何防止过拟合</li>
<li>lr和树模型，离散特征和连续特征分别怎么处理</li>
<li>lr和线性回归的区别</li>
<li>连续特征可以直接输入到lr中不? （归一化和标准化有什么区别）</li>
<li>线性回归可以求闭式解，逻辑回归可以吗，为什么，LR用什么求解参数，为什么要用梯度下降算法</li>
<li>SVM和LR的区别</li>
<li>SVM的公式会推导嘛，SVM的损失函数</li>
<li>SVM原理，为什么求最大间隔，为什么用核函数，常见的核函数及区别</li>
<li>SVM支持向量怎么得到的</li>
<li>写一下SVM的原问题和对偶问题，分别解释一下</li>
<li>SVM核函数有什么性质，写一下SVM核化的形式</li>
<li>无监督学习，半监督学习，有监督学习的区别</li>
<li>有哪些无监督学习的方法（kmeans,pca,生成模型，自编码器）</li>
<li>有哪些回归模型（多项式回归，树模型，svr, 神经网络）</li>
<li>生成模型、判别模型的区别</li>
<li>概率和似然的区别</li>
<li>最大似然估计和后验概率的区别，分别用LR来推导损失函数的话有什么区别（乘以W的先验概率）</li>
<li>朴素贝叶斯介绍，朴素贝叶斯公式，为什么朴素</li>
<li>l1,l2特性及原理，分别适用于那些场合</li>
<li>给一个多峰数据场景，为什么l2不适合，可以怎么解决</li>
<li>讲讲Kmeans、EM算法</li>
<li>机器学习中怎么解决过拟合，DNN中怎么解决</li>
<li>说一下SVD怎么降维</li>
<li>推导softmax做激活函数求导</li>
<li>LR，SVM,xgb哪个对样本不平衡不太敏感，顺便把SVM和xgb介绍了</li>
<li>降维方法了解嘛，PCA? 为什么取特征值前k大的对应的特征向量组成的矩阵？低秩表示</li>
</ul>
<h2 id="1-2-深度学习"><a href="#1-2-深度学习" class="headerlink" title="1.2 深度学习"></a>1.2 深度学习</h2><ul>
<li>梯度是什么，hessian矩阵怎么求</li>
<li>有没有上过凸优化的课程，如何判断凸函数</li>
<li>防止过拟合的策略有哪些</li>
<li>dropout怎么防止过拟合, Dropout在训练和测试区别</li>
<li>BN介绍，BN怎么防止过拟合，怎么用的，参数量, 参数怎么得到的</li>
<li>优化器，SGD与Adam的异同点</li>
<li>介绍一下你了解的激活函数以及优缺点</li>
<li>介绍一下深度学习的优化器发展史, 及常见的优化算法</li>
<li>梯度爆炸和梯度消失问题</li>
<li>SGD缺点，已经有什么改进的优化器</li>
<li>网络权重初始化为0有什么影响，初始化为一个非0的常数呢？</li>
<li>embedding如何设置维度？越大越好还是越小越好？</li>
<li>transformer中计算attention除于根号d的作用</li>
<li>embedding如何训练</li>
<li>介绍下attention，相比cnn、lstm的优势</li>
<li>word2vec如何进行负采样</li>
<li>word2vec两种训练方法的区别，具体损失函数</li>
<li>介绍LSTM每一个门的具体操作，一个LSTM cell的时间复杂度是多少</li>
<li>transformer中encoder和decoder的输入分别是什么</li>
<li>transformer中encoder与decoder的QKV矩阵如何产生</li>
<li>transformer中QKV矩阵是否可以设置成同一个</li>
<li>transformer与bert的位置编码有什么区别</li>
<li>BERT中计算attention的公式</li>
<li>BERT中LayerNorm的作用，为什么不用BN？</li>
<li>BERT中的两种预训练任务介绍</li>
<li>深度学习中BN的好处？最早提出BN是为了解决什么问题？BN具体怎么实现的</li>
<li>激活函数中，sigmoid，tanh有什么不好的地方？relu有什么优势？</li>
<li>pagerank相比于tf-idf的优势</li>
<li>画一下LSTM的结构图</li>
<li>RNN和LSTM的区别，解决了什么问题，为什么解决了梯度消失的问题</li>
<li>深度模型和传统机器学习模型对数据量的要求，什么场景用什么模型</li>
</ul>
<h2 id="1-3-特征工程"><a href="#1-3-特征工程" class="headerlink" title="1.3 特征工程"></a>1.3 特征工程</h2><h2 id="1-4-评估指标"><a href="#1-4-评估指标" class="headerlink" title="1.4 评估指标"></a>1.4 评估指标</h2><h1 id="2-推荐模型相关"><a href="#2-推荐模型相关" class="headerlink" title="2 推荐模型相关"></a>2 推荐模型相关</h1><h2 id="2-1-召回"><a href="#2-1-召回" class="headerlink" title="2.1 召回"></a>2.1 召回</h2><h2 id="2-2-排序"><a href="#2-2-排序" class="headerlink" title="2.2 排序"></a>2.2 排序</h2><h1 id="3-热门技术相关"><a href="#3-热门技术相关" class="headerlink" title="3 热门技术相关"></a>3 热门技术相关</h1><h2 id="3-1-Embedding"><a href="#3-1-Embedding" class="headerlink" title="3.1 Embedding"></a>3.1 Embedding</h2><h2 id="3-2-多任务学习"><a href="#3-2-多任务学习" class="headerlink" title="3.2 多任务学习"></a>3.2 多任务学习</h2><h2 id="3-3-图神经网络"><a href="#3-3-图神经网络" class="headerlink" title="3.3 图神经网络"></a>3.3 图神经网络</h2><h1 id="4-业务场景相关"><a href="#4-业务场景相关" class="headerlink" title="4 业务场景相关"></a>4 业务场景相关</h1><h1 id="5-HR及其他"><a href="#5-HR及其他" class="headerlink" title="5 HR及其他"></a>5 HR及其他</h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">expAddThree</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/12/09/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/">http://example.com/2024/12/09/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">LXY's Notes</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%9D%A2%E8%AF%95/">面试</a><a class="post-meta__tags" href="/tags/%E6%90%9C%E5%B9%BF%E6%8E%A8/">搜广推</a></div><div class="post-share"><div class="social-share" data-image="/img/kanna.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-left" href="/2024/12/18/hot100/" title="hot100"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">hot100</div></div></a><a class="next-post pull-right" href="/2024/12/09/Leetcode-Day18/" title="Leetcode-Day18"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Leetcode-Day18</div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/img/kanna.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info-name">expAddThree</div><div class="author-info-description">5L2g5aW977yBCg==</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/expAdd3"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:lixybupt@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">An announcement means no announcement! (^_^)</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-ML%E4%B8%8EDL%E5%9F%BA%E7%A1%80"><span class="toc-text">1 ML与DL基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.1 机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-1-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%AA%E6%9C%80%E7%86%9F%E6%82%89%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-text">1.1.1 介绍一个最熟悉的机器学习算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-2-%E5%86%B3%E7%AD%96%E6%A0%91%E6%80%8E%E4%B9%88%E5%BB%BA%E6%A0%91%EF%BC%8C%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0%E5%85%AC%E5%BC%8F"><span class="toc-text">1.1.2 决策树怎么建树，基尼系数公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-3-Adaboost%E6%8B%9F%E5%90%88%E7%9B%AE%E6%A0%87%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-text">1.1.3 Adaboost拟合目标是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-4-Adaboost%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%EF%BC%8C%E6%AF%8F%E4%B8%AA%E5%9F%BA%E5%AD%A6%E4%B9%A0%E5%99%A8%E7%9A%84%E6%9D%83%E9%87%8D%E6%80%8E%E4%B9%88%E5%BE%97%E5%88%B0%E7%9A%84"><span class="toc-text">1.1.4 Adaboost介绍一下，每个基学习器的权重怎么得到的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-5-%E4%BB%8B%E7%BB%8D%E4%B8%8BGBDT"><span class="toc-text">1.1.5 介绍下GBDT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-5-1-%E4%B8%BA%E4%BB%80%E4%B9%88GBDT%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%9B%9E%E5%BD%92%E6%A0%91%E8%80%8C%E4%B8%8D%E6%98%AF%E5%88%86%E7%B1%BB%E6%A0%91%EF%BC%9F"><span class="toc-text">1.1.5.1 为什么GBDT中使用回归树而不是分类树？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-6-%E4%BB%8B%E7%BB%8DXGBoost"><span class="toc-text">1.1.6 介绍XGBoost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-7-%E4%BB%8B%E7%BB%8D%E4%B8%8BLightGBM"><span class="toc-text">1.1.7 介绍下LightGBM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-7-1-LightGBM%E6%A0%B8%E5%BF%83%E7%89%B9%E7%82%B9"><span class="toc-text">1.1.7.1 LightGBM核心特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-7-2-LightGBM%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0"><span class="toc-text">1.1.7.2 LightGBM主要技术创新</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-7-LightGBM%E7%9B%B8%E5%AF%B9%E4%BA%8EXGBoost%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="toc-text">1.1.7. LightGBM相对于XGBoost的改进</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-8-GBDT%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E6%80%8E%E4%B9%88%E7%94%A8"><span class="toc-text">1.1.8 GBDT中的梯度是什么，怎么用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-9-GBDT%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-text">1.1.9 GBDT如何计算特征重要性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-10-GBDT%E8%AE%B2%E4%B8%80%E4%B8%8B%EF%BC%8CGBDT%E6%8B%9F%E5%90%88%E6%AE%8B%E5%B7%AE%EF%BC%8C%E6%98%AF%E7%9C%9F%E5%AE%9E%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%98%9B%EF%BC%8C%E5%9C%A8%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E4%B8%8B%E7%9C%8B%E5%81%9A%E6%98%AF%E7%9C%9F%E5%AE%9E%E7%9A%84%E8%AF%AF%E5%B7%AE"><span class="toc-text">1.1.10 GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-11-%E4%BB%8B%E7%BB%8DXGBoost%E4%B8%AD%E7%9A%84%E5%B9%B6%E8%A1%8C"><span class="toc-text">1.1.11 介绍XGBoost中的并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-12-%E4%BB%8B%E7%BB%8DXGBoost%E4%B8%AD%E7%B2%BE%E7%A1%AE%E7%AE%97%E6%B3%95%E4%B8%8E%E8%BF%91%E4%BC%BC%E7%AE%97%E6%B3%95"><span class="toc-text">1.1.12 介绍XGBoost中精确算法与近似算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-13-XGBoost%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%A9%BA%E7%BC%BA%E5%80%BC%EF%BC%8C%E4%B8%BA%E4%BD%95%E8%A6%81%E8%BF%9B%E8%A1%8C%E8%A1%8C%E9%87%87%E6%A0%B7%E3%80%81%E5%88%97%E9%87%87%E6%A0%B7"><span class="toc-text">1.1.13 XGBoost如何处理空缺值，为何要进行行采样、列采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-14-%E8%AE%B2%E4%B8%80%E4%B8%8Bxgboost%E7%AE%97%E6%B3%95%EF%BC%8Cxgboost%E6%98%AF%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%A6%BB%E6%95%A3%E7%89%B9%E5%BE%81%E7%9A%84%EF%BC%8Cxgb%E6%80%8E%E4%B9%88%E8%AE%AD%E7%BB%83%EF%BC%8Cxgb%E7%AE%97%E6%B3%95%E4%BC%98%E7%82%B9%EF%BC%8C%E6%80%8E%E4%B9%88%E9%80%89%E7%89%B9%E5%BE%81%EF%BC%8C%E4%B8%BB%E8%A6%81%E5%8F%82%E6%95%B0%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%8Cxgb%E7%9A%84%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E6%80%8E%E4%B9%88%E7%9C%8B"><span class="toc-text">1.1.14 讲一下xgboost算法，xgboost是如何处理离散特征的，xgb怎么训练，xgb算法优点，怎么选特征，主要参数有哪些，xgb的特征重要性怎么看</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-15-xgboost%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%EF%BC%8Cxgb%E5%AF%B9%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%BA%8C%E9%98%B6%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%EF%BC%8C%E5%93%AA%E4%B8%AA%E6%98%AFx%EF%BC%8C%E5%93%AA%E4%B8%AA%E6%98%AFdelta-x-%E4%B8%80%E9%98%B6%E5%AF%BC%E5%92%8C%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%98%AF%E5%AF%B9%E8%B0%81%E6%B1%82%E5%BE%97"><span class="toc-text">1.1.15 xgboost介绍一下，xgb对目标函数二阶泰勒展开，哪个是x，哪个是delta x, 一阶导和二阶导是对谁求得</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-16-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%AB%98%E7%BB%B4%E7%A8%80%E7%96%8F%E6%95%B0%E6%8D%AE%EF%BC%8CLR%E6%AF%94GBDT%E8%A6%81%E5%A5%BD"><span class="toc-text">1.1.16 为什么高维稀疏数据，LR比GBDT要好</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-17-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%8EGBDT%E9%87%87%E6%A0%B7%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">1.1.17 随机森林与GBDT采样的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-18-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%AD%E5%88%97%E9%87%87%E6%A0%B7%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-text">1.1.18 随机森林中列采样的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-19-bagging%E4%B8%8Eboosting%E5%AF%B9%E6%AF%94-boosting%E5%92%8Cbagging%E7%9A%84%E5%8C%BA%E5%88%AB%E5%8F%8A%E5%88%86%E5%88%AB%E9%80%82%E7%94%A8%E4%BA%8E%E4%BB%80%E4%B9%88%E5%9C%BA%E6%99%AF"><span class="toc-text">1.1.19 bagging与boosting对比, boosting和bagging的区别及分别适用于什么场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-20-bagging%E4%B8%8Eboosting%E5%88%86%E5%88%AB%E4%BB%8E%E4%BB%80%E4%B9%88%E8%A7%92%E5%BA%A6%E9%99%8D%E4%BD%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">1.1.20 bagging与boosting分别从什么角度降低过拟合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.2 深度学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-text">1.3 特征工程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-text">1.4 评估指标</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3"><span class="toc-text">2 推荐模型相关</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%8F%AC%E5%9B%9E"><span class="toc-text">2.1 召回</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E6%8E%92%E5%BA%8F"><span class="toc-text">2.2 排序</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E7%83%AD%E9%97%A8%E6%8A%80%E6%9C%AF%E7%9B%B8%E5%85%B3"><span class="toc-text">3 热门技术相关</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Embedding"><span class="toc-text">3.1 Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0"><span class="toc-text">3.2 多任务学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">3.3 图神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF%E7%9B%B8%E5%85%B3"><span class="toc-text">4 业务场景相关</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-HR%E5%8F%8A%E5%85%B6%E4%BB%96"><span class="toc-text">5 HR及其他</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/18/hot100/" title="hot100">hot100</a><time datetime="2024-12-18T13:06:39.000Z" title="发表于 2024-12-18 21:06:39">2024-12-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/09/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/" title="推荐算法面试">推荐算法面试</a><time datetime="2024-12-09T05:07:50.000Z" title="发表于 2024-12-09 13:07:50">2024-12-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/09/Leetcode-Day18/" title="Leetcode-Day18">Leetcode-Day18</a><time datetime="2024-12-09T02:06:53.000Z" title="发表于 2024-12-09 10:06:53">2024-12-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/11/Dive%20Into%20Deep%20Learning/" title="Dive Into Deep Learning">Dive Into Deep Learning</a><time datetime="2024-11-11T06:48:59.000Z" title="发表于 2024-11-11 14:48:59">2024-11-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/10/Leetcode-Day17/" title="Leetcode-Day17">Leetcode-Day17</a><time datetime="2024-11-10T04:17:14.000Z" title="发表于 2024-11-10 12:17:14">2024-11-10</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">©2019 - 2024 By expAddThree</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>