<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习 | LXY's Notes</title><meta name="author" content="expAddThree"><meta name="copyright" content="expAddThree"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深度学习1 机器学习的流程 数据获取 特征工程 * 建立模型 评估与应用  特征工程的作用：  数据特征决定了模型的上限 预处理和特征提取是最核心的 算法和参数的选择决定了如何逼近这个上限  2 深度学习的应用结合计算机视觉或者自然语言处理的应用  3 计算机视觉任务 图像分类  假如判断这个是个啥玩意，下面这个300100\3的意思就是，该图像的大小是300像素高、100像素宽，并且每个像素由三">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习">
<meta property="og:url" content="http://example.com/2024/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="LXY's Notes">
<meta property="og:description" content="深度学习1 机器学习的流程 数据获取 特征工程 * 建立模型 评估与应用  特征工程的作用：  数据特征决定了模型的上限 预处理和特征提取是最核心的 算法和参数的选择决定了如何逼近这个上限  2 深度学习的应用结合计算机视觉或者自然语言处理的应用  3 计算机视觉任务 图像分类  假如判断这个是个啥玩意，下面这个300100\3的意思就是，该图像的大小是300像素高、100像素宽，并且每个像素由三">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/kanna.jpg">
<meta property="article:published_time" content="2024-10-04T10:34:43.000Z">
<meta property="article:modified_time" content="2024-10-04T11:16:37.468Z">
<meta property="article:author" content="expAddThree">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/kanna.jpg"><link rel="shortcut icon" href="/img/myIcon.png"><link rel="canonical" href="http://example.com/2024/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":-1,"unescape":true,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-04 19:16:37'
}</script><link rel="stylesheet" href="/css/modify.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/background-1.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/kanna.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">55</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">LXY's Notes</span></a><a class="nav-page-title" href="/"><span class="site-name">深度学习</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">深度学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-04T10:34:43.000Z" title="发表于 2024-10-04 18:34:43">2024-10-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-04T11:16:37.468Z" title="更新于 2024-10-04 19:16:37">2024-10-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/DeepLearnng/">DeepLearnng</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">6.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><h4 id="1-机器学习的流程"><a href="#1-机器学习的流程" class="headerlink" title="1 机器学习的流程"></a>1 机器学习的流程</h4><ol>
<li>数据获取</li>
<li>特征工程 <strong>*</strong></li>
<li>建立模型</li>
<li>评估与应用</li>
</ol>
<p>特征工程的作用：</p>
<ul>
<li>数据特征决定了模型的上限</li>
<li>预处理和特征提取是最核心的</li>
<li>算法和参数的选择决定了如何逼近这个上限</li>
</ul>
<h4 id="2-深度学习的应用"><a href="#2-深度学习的应用" class="headerlink" title="2 深度学习的应用"></a>2 深度学习的应用</h4><p>结合计算机视觉或者自然语言处理的应用 </p>
<h4 id="3-计算机视觉任务"><a href="#3-计算机视觉任务" class="headerlink" title="3 计算机视觉任务"></a>3 计算机视觉任务</h4><ul>
<li>图像分类</li>
</ul>
<p>假如判断这个是个啥玩意，下面这个300<em>100\</em>3的意思就是，该图像的大小是300像素高、100像素宽，并且每个像素由三个颜色通道组成，也就是RGB通道。比如说第一个像素image[0][0]=[255.0.0]，也就是红色。</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-13_09-53-02.png" style="zoom:67%;"></p>
<p>但是这个存在照射角度、形状改变、部分这笔以及背景混入（类似于保护色那种）等的挑战。</p>
<p>机器学习常规的套路：</p>
<ul>
<li>收集数据并给定标签</li>
<li>训练一个分类器</li>
<li>测试评估</li>
</ul>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-14_08-43-58.png" style="zoom:67%;"></p>
<h4 id="4-K近邻算法"><a href="#4-K近邻算法" class="headerlink" title="4 K近邻算法"></a>4 K近邻算法</h4><p>数据：两类点：方块和三角</p>
<p>问绿色的点属于方块还是三角呢？</p>
<p>就是看一下周围什么多他就是什么，人以类聚。</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-14_08-46-54.png" style="zoom:50%;"></p>
<p>K=3和K=5也不一样，K=X，就是看周围的X个，K=3那么他就是三角，K=5他就是方块。</p>
<p><strong>K近邻计算流程</strong></p>
<ol>
<li>计算已知类别数据集中的点与当前点的距离</li>
<li>按照距离依次排序</li>
<li>选取与当前点距离最小的K个点</li>
<li>确定前K个点所在类别的出现概率</li>
<li>返回前K个点出现频率最高的类别作为当前点预测分类</li>
</ol>
<p><strong>K近邻分析</strong></p>
<ul>
<li>KNN 算法本身简单有效，它是一种 lazy-learning 算法。</li>
<li>分类器不需要使用训练集进行训练，训练时间复杂度为0。</li>
<li>KNN 分类的计算复杂度和训练集中的文档数目成正比，</li>
<li>也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。</li>
<li>K 值的选择，距离度量和分类决策规则是该算法的三个基本要素</li>
</ul>
<p>距离的选择</p>
<p>像素点对应相减   </p>
<p>L1（Manhattan） distance:</p>
<script type="math/tex; mode=display">
d_1(I_1,I_2)=\Sigma_P{|I_1^P-I_2^P|}</script><img src="/2024/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Snipaste_2024-08-14_08-55-25.png" class="">
<p>L2 （Euclidean）distance</p>
<script type="math/tex; mode=display">
d_1(I_1,I_2)=\sqrt{\Sigma_P{(I_1^P-I_2^P)}^2}</script><p>对于K近邻来说，参数对结果影响很大，交叉验证是最合适的参数选择方法。</p>
<p>K近邻交叉验证：</p>
<p>K近邻算法（K-Nearest Neighbor, KNN）是一种常用的分类和回归方法。在KNN中，交叉验证是一种重要的技术，用于评估模型的性能和泛化能力，以及帮助选择最佳的模型参数，如K值（即最近邻的数量）。以下是K近邻算法进行交叉验证的基本步骤：</p>
<ol>
<li><p><strong>数据集划分</strong>：</p>
<ul>
<li>首先，将原始数据集划分为训练集和测试集。但在交叉验证中，通常不会显式地划分出独立的测试集，因为交叉验证本身就是为了在训练集上多次划分和评估来模拟测试集的效果。</li>
<li>在K折交叉验证（K-fold cross-validation）中，数据集被均匀分成K个子集（或称为“折”）。每个子集在交叉验证的过程中轮流作为测试集，其余的子集作为训练集。</li>
</ul>
</li>
<li><p><strong>多次训练和测试</strong>：</p>
<ul>
<li>执行K次训练和测试。在每次迭代中，选择一个不同的子集作为测试集，其余的子集合并作为训练集。</li>
<li>使用训练集训练KNN模型，并使用测试集评估模型的性能。性能评估指标可以是准确率、F1分数等，具体取决于问题的性质。</li>
</ul>
</li>
<li><p><strong>计算性能指标</strong>：</p>
<ul>
<li>每次迭代后，都会得到一个性能指标值。在所有K次迭代完成后，计算这些性能指标值的平均值，作为模型的最终性能指标。</li>
<li>这个平均值能够更准确地反映模型在未知数据上的表现，因为它考虑了模型在不同子集上的性能。</li>
</ul>
</li>
<li><p><strong>选择最佳参数</strong>：</p>
<ul>
<li>如果在进行交叉验证时还涉及参数调优（如选择最佳的K值），则可以在不同的K值下重复上述步骤，并比较不同K值下的性能指标。</li>
<li>选择性能指标最高的K值作为最佳参数，并基于该参数重新训练整个数据集上的模型。</li>
</ul>
</li>
<li><p><strong>注意事项</strong>：</p>
<ul>
<li>当K等于原始数据集的大小时，这种交叉验证方法称为留一法（Leave-One-Out, LOO）。但LOO计算代价较高，可能不是所有情况下的首选方法。</li>
<li>交叉验证中的子集会重复使用，以确保模型在不同数据集上的性能评估是全面的，并减少因数据集划分不合理而引入的偶然性。</li>
</ul>
</li>
</ol>
<p>在Python的scikit-learn库中，可以方便地使用<code>cross_val_score</code>函数来进行交叉验证。例如：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设X是特征数据，y是目标数据</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)  <span class="comment"># 初始化KNN模型，n_neighbors为K值</span></span><br><span class="line">scores = cross_val_score(knn, X, y, cv=<span class="number">5</span>)  <span class="comment"># 进行5折交叉验证</span></span><br><span class="line"><span class="built_in">print</span>(scores.mean())  <span class="comment"># 打印平均性能指标</span></span><br></pre></td></tr></tbody></table></figure>
<p>这段代码会输出KNN模型在5折交叉验证下的平均性能指标，帮助评估模型的性能和泛化能力。</p>
<h4 id="5-神经网络基础"><a href="#5-神经网络基础" class="headerlink" title="5 神经网络基础"></a>5 神经网络基础</h4><h5 id="5-1-得分函数（线性函数）"><a href="#5-1-得分函数（线性函数）" class="headerlink" title="5.1 得分函数（线性函数）"></a>5.1 得分函数（线性函数）</h5><p>从输入到输出的映射</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-14_09-21-20.png" style="zoom:60%;"></p>
<blockquote>
<p>上图中这个得分就是比如输入狗的得分，属于猫的得分等等。</p>
</blockquote>
<p> 数学表示：</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-14_10-03-16.png" style="zoom:50%;"></p>
<p>其中</p>
<ul>
<li>W：权重参数，对结果起决定因素</li>
<li>b：偏执参数，对结果微调</li>
</ul>
<p>这里面这个b是10x1是因为前面的Wx算下来得到的矩阵也是10x1的，他对十个不同的类别都进行微调</p>
<p>举个例子，比如这个猫分为了四个像素点 </p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-14_10-12-19.png" style="zoom:55%;"></p>
<p>左边的权重矩阵是一步步优化来的，识别成狗的原因是W不好，与x无关，x又不会变。对于神经网络就是选择一个合适的W。</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-14_10-17-31.png" style="zoom:60%;"></p>
<h5 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 损失函数</h5><p>损失函数有很多种，对于上面的背景，来演示一个</p>
<script type="math/tex; mode=display">
L_i=\Sigma_{j\neq{y_i}}max(0,s_j-s_{y_i}+1)</script><p><img src="/2024/10/04/深度学习/Snipaste_2024-08-14_10-25-21.png" style="zoom:60%;"></p>
<p>其中:</p>
<script type="math/tex; mode=display">
\begin{flalign}
& s_j 为其他类别 \\
& s_{y_i} 为正确类别 &
\end{flalign}</script><p>公式也就是说正确类别比其他区类别多1才算是没有损失的，类似于一个容忍程度。最终得到的值越大损失越大。</p>
<p>损失函数得到的值相同，不意味着两个模型一样。</p>
<p>损失函数可以改进为：损失函数 = 数据损失 + 正则化惩罚项</p>
<script type="math/tex; mode=display">
L=\frac{1}{N}\Sigma{^{N}_{i=1}}\Sigma_{j\neq{y_i}}max((0,f{x_i;W})_{j}-f(x_i;W)_{y_i}+1)+\lambda R(W)</script><ul>
<li><em>λR(W)</em>就是正则化惩罚项，由权重参数带来的损失，λ是惩罚系数，比较大就是不要变异的，通常情况下希望模型不要太复杂，过拟合的模型是没用的。</li>
</ul>
<script type="math/tex; mode=display">
R(W)=\Sigma_k\Sigma_lW^2_{k,j}</script><ul>
<li>前半部分数据损失(data-loss)，数据在当前损失函数中得到的损失</li>
</ul>
<p>什么是过拟合(Overfitting)的模型呢？</p>
<blockquote>
<p>过拟合是机器学习中的一个常见问题，指的是模型在训练数据上表现得非常好，甚至能够近乎完美的拟合训练数据，但是在未见过的测试数据或者实际应用中却表现不佳。也就是说过拟合的模型学习到了训练数据中的噪声和异常值，而非数据的真实分布或潜在规律，所以它无法泛化到新的数据上，出现的原因包括：</p>
<ul>
<li>模型复杂度过高</li>
<li>训练数据不足</li>
<li>训练时间过长</li>
<li>特征选择不当</li>
</ul>
</blockquote>
<h5 id="5-3-Softmax分类器"><a href="#5-3-Softmax分类器" class="headerlink" title="5.3 Softmax分类器"></a>5.3 Softmax分类器</h5><p>现在得到的是一个输入的得分值，给概率不是更好吗。</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-14_10-52-49.png" style="zoom:60%;"></p>
<ul>
<li>exp：e的x次幂，这个x就是左侧的得分，为的是放大差异，让结果更明显</li>
<li>normalize：归一化，就是计算每一个占比，比如cat的24.5，就是24.5/（三个的和）</li>
<li>得到了占比，怎么计算损失呢，使用log函数，只关注当前输入属于正确类别得概率值，为什么加负号，是因为概率都在0-1之前，log都是负的，根据log得函数来看，概率越接近于1，损失越小，相反损失越大。但是这边计算损失值存在负号，所以概率越小，得到的结果越靠近1，损失就越大。</li>
</ul>
<p>由x和W计算得到loss叫做前向传播，反向传播是怎么调节W让loss下降</p>
<h4 id="6-反向传播"><a href="#6-反向传播" class="headerlink" title="6 反向传播"></a>6 反向传播</h4><p>链式法则</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_08-10-31.png" style="zoom:60%;"></p>
<ul>
<li>L指的是loss</li>
</ul>
<p>机器学习的套路就是我交给机器一堆数据，然后告诉它什么样的学习方式是对的（目标函数），然后让它朝着这个方向去做。</p>
<p>计算就是比如</p>
<script type="math/tex; mode=display">
Loss=W_3(W_2(W_1(q)))</script><p>然后反向传播一层一侧计算偏导，先对W3求偏导计算其对loss的影响，再接着求W2（当然这其中没有写非线性变换），然后一层层更新，找到最合适的权重参数。</p>
<h5 id="6-1-梯度下降"><a href="#6-1-梯度下降" class="headerlink" title="6.1 梯度下降"></a>6.1 梯度下降</h5><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1oY411N7Xz/?spm_id_from=333.337.search-card.all.click&amp;vd_source=cad7792963e5468a03db9ab0feb759ff">[5分钟深度学习] #01 梯度下降算法_哔哩哔哩_bilibili</a></p>
<h4 id="7-神经网络"><a href="#7-神经网络" class="headerlink" title="7 神经网络"></a>7 神经网络</h4><p>给个图</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_09-23-06.png" style="zoom: 80%;"></p>
<p>介绍</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_08-34-20.png" style="zoom:60%;"></p>
<ul>
<li>层次结构：下面的各种层，一层一层来变换数据</li>
<li>神经元：数据的量（矩阵的大小）。比如图中input层有多少圈，就代表输入特征有多少个</li>
<li>全连接：前一层会和后一层的所有特征相连。比如说hidden1层就是将原始的特征转换成我能理解的特征</li>
<li>非线性：</li>
</ul>
<blockquote>
<p>其实这些线就是权重参数W </p>
</blockquote>
<p>每次Wx计算之后，会进行一个非线性计算，再与下一个W进行计算</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_08-52-46.png" style="zoom:60%;"></p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_08-52-55.png" style="zoom:80%;"></p>
<p>这里面的max就是非线性函数。</p>
<h5 id="7-1-神经元个数对结果的影响"><a href="#7-1-神经元个数对结果的影响" class="headerlink" title="7.1 神经元个数对结果的影响"></a>7.1 神经元个数对结果的影响</h5><p>神经网络的强大之处在于，用更多的参数来拟合复杂的数据。</p>
<p>神经元数量在一定范围内越多得到的结果越好，但是不能太大，太大会出现过拟合现象。</p>
<h5 id="7-2-正则化与激活函数"><a href="#7-2-正则化与激活函数" class="headerlink" title="7.2  正则化与激活函数"></a>7.2  正则化与激活函数</h5><p>正则化惩罚力度对结果的影响，越小的λ越符合训练结果。</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_09-11-18.png" style="zoom:60%;"></p>
<p>参数个数对结果的影响，X个隐藏神经元就是想由输入的特征得到多少维的其他特征。（64，128，256）</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_09-14-51.png" style="zoom:60%;"></p>
<p>常用的激活函数（非线性变换）：Sigmoid、ReLU、Tanh等</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_09-26-57.png" style="zoom:60%;"></p>
<ul>
<li>Sigmoid函数适合用于二分类问题的输出层，ReLU则因其简单性和缓解梯度消失问题而被广泛使用。</li>
</ul>
<blockquote>
<p>梯度消失问题（Gradient Vanishing Problem）是深度神经网络训练过程中常见的一个问题，特别是在使用基于梯度的优化算法（如反向传播算法）时。这个问题指的是在神经网络的较深层（即靠近输入层的层，因为梯度是从输出层反向传播到输入层的）中，梯度逐渐变得非常小，甚至接近于零，导致这些层的参数更新变得非常缓慢或几乎停止，从而使得这些层无法有效地学习到数据的特征。</p>
<p>举个例子来说明梯度消失问题：</p>
<p>假设我们有一个具有多层隐藏层的神经网络，每个隐藏层都使用某种激活函数（如Sigmoid函数）。在训练过程中，我们使用反向传播算法来计算每一层的梯度，并根据这些梯度来更新每一层的参数。然而，由于Sigmoid函数的导数在输入值接近其极值（0或1）时趋近于0，因此在深层网络中，如果输入值经过多次Sigmoid函数变换后变得非常接近0或1，那么这些层的梯度就会因为多次乘以接近0的数而变得非常小，导致梯度消失。</p>
<p>具体来说，假设我们有一个四层隐藏层的神经网络，每层都使用Sigmoid激活函数，并且每层的梯度都小于1（例如，Sigmoid函数的导数在最大值处为0.25）。那么，当梯度从输出层反向传播到第一层时，它会被乘以四层的梯度（每层都小于1），导致最终的梯度变得非常小。如果网络层数更多，这个问题就会更加严重。</p>
<p>梯度消失问题会导致网络训练速度变慢，甚至无法收敛到最优解。为了解决这个问题，人们提出了许多方法，如使用ReLU等新的激活函数、采用批标准化（Batch Normalization）技术、设计更合理的网络结构（如残差网络ResNet）等。</p>
</blockquote>
<ul>
<li>Sigmoid就存在梯度消失问题，因为在这个函数中一旦数值较大或者数值较小，得到的结果就不好了，几乎没啥变化。（见上图）对于ReLU因为小于0的直接剔除掉了，大于0的梯度为本身，所以没有梯度消失问题。</li>
</ul>
<h5 id="7-3-数据预处理"><a href="#7-3-数据预处理" class="headerlink" title="7.3 数据预处理"></a>7.3 数据预处理</h5><p>拿到数据之后要先变换一下再放到神经网络处理。</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_09-39-26.png" style="zoom:60%;"></p>
<p>比如说先把原始数据中心化，每个位置实际坐标值-均值，在对各个维度进行放缩，每个位置坐标 / 标准差</p>
<h5 id="7-4-参数初始化"><a href="#7-4-参数初始化" class="headerlink" title="7.4 参数初始化"></a>7.4 参数初始化</h5><p>通常使用随机策略来进行参数初始化</p>
<script type="math/tex; mode=display">
W=0.01*np.random.randn(D,H)</script><p>就是采用随机策略来生成权重参数。</p>
<ul>
<li>0.01 就是控制权重参数不要浮动很大</li>
</ul>
<h5 id="7-5-DROP-OUT"><a href="#7-5-DROP-OUT" class="headerlink" title="7.5 DROP-OUT"></a>7.5 DROP-OUT</h5><p>降低神经网络过拟合风险，舍弃神经网络中的一部分，在每一层随机的杀死一部分神经元（固定比例），就是前向传播和反向传播就没它事了，也不是说永远不用，他每次训练舍弃的神经元不一样，是随机的。</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-15_09-45-29.png" style="zoom:60%;"></p>
<p>测试阶段不杀死，是都参与的。所以DROP-OUT就是在训练阶段防止模型太复杂。</p>
<h4 id="8-卷积神经网络"><a href="#8-卷积神经网络" class="headerlink" title="8 卷积神经网络"></a>8 卷积神经网络</h4><p>卷积神经网络（CNN，Convolutional Neural Network）就是应用到计算机视觉当中的。</p>
<p>应用场景：</p>
<ul>
<li>检测任务</li>
<li>分类与检索（比如识别相似物品，百度识图那种）</li>
<li>超分辨率重构（比如让图像更清晰）</li>
<li>医学任务等（车牌啥的）</li>
<li>无人驾驶</li>
<li>人脸识别</li>
</ul>
<h5 id="8-1-卷积网络与传统网络的区别"><a href="#8-1-卷积网络与传统网络的区别" class="headerlink" title="8.1 卷积网络与传统网络的区别"></a>8.1 卷积网络与传统网络的区别</h5><p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_08-04-10.png" style="zoom:60%;"></p>
<p>举个例子，传统的神经网络输入的是像素点的个数（一列特征），比如784个（28*28*1），但是CNN输入的是28*28*1（像是三维），长方体的矩阵 h*w*c</p>
<h5 id="8-2-CNN整体架构"><a href="#8-2-CNN整体架构" class="headerlink" title="8.2 CNN整体架构"></a>8.2 CNN整体架构</h5><p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_08-07-57.png" style="zoom:60%;"></p>
<p>卷积提取特征，池化压缩特征</p>
<h6 id="8-2-1-卷积"><a href="#8-2-1-卷积" class="headerlink" title="8.2.1 卷积"></a>8.2.1 卷积</h6><p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_08-09-30.png" style="zoom:60%;"></p>
<p>将原始数据进行分割成多个小区域，因为图片的每个地方特征值可能不同，比如拿出来一个5*5*3的区域，我需要拿到它的特征值，怎么算呢，拿上图举例，它选出来一个3*3的区域，然后根据该区域的权重参数矩阵，也就是右下角小的这个，来计算特征值（上图这个例子举得不好，这只演示了一个通道），图上得到的特征12是该小区域与卷积核（权重矩阵）的乘积，是对应相乘求和（所有卷积网络都是这样的内积计算）</p>
<script type="math/tex; mode=display">
a_{11}*b_{11}+a_{12}*b_{12}+...+a_{33}*b_{33}</script><h6 id="8-2-2-图像颜色通道"><a href="#8-2-2-图像颜色通道" class="headerlink" title="8.2.2 图像颜色通道"></a>8.2.2 图像颜色通道</h6><p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_08-35-38.png" style="zoom:80%;"></p>
<p>实际上进行计算时，需要对每个通道单独进行计算，再将每个通道卷积完的结果加在一起</p>
<p>所以卷积来说：</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_08-41-33.png" style="zoom:60%;"></p>
<p>对于上图，给RGB三个通道对应的位置分别计算结果，然后再加起来（当然这个地方可能还需要加一个偏执项），就是最终的对应位置的特征。</p>
<p>需要注意的是，输入的数据的维度，也就是h*w*c中的c要与卷积核中的维度c相同，就是说假如是RGB的图像，维度是3，那么每一次的卷积核也需要有3组。那么卷积核的h*w代表要在原始数据中的每多大区域选出来一个特征。</p>
<p>可以定义多个不同的卷积核filter，每有一个就会生成一个特征图，比如下图，使用了两个卷积核，生成了两个特征图</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_08-54-40.png" style="zoom:60%;"></p>
<p>下图生成了6个28*28*1的特征图，最终堆叠在一起，就变成了28*28*6</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_08-55-44.png" style="zoom:60%;"></p>
<p>注意：</p>
<ul>
<li>同一个卷积层选的卷积核的大小必须是一样的，比如卷积层1卷积核为3*3*3，但是我在卷积层2就可以设置为4*4*3。</li>
<li>计算完一个区域，移动到下一个区域移动的数量（滑动窗口步长）是可以自己定的，但是每次要一样</li>
</ul>
<p>对于一张输入数据，做一次卷积是不够的，需要做好多次，他并不是在一个输入图像上做多次平行的卷积，它是在上次生成的卷积图上再做卷积</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_09-09-34.png" style="zoom:60%;"></p>
<p>卷积结果计算：上图怎么从32到28的</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_10-40-46.png" style="zoom:70%;"></p>
<h6 id="8-2-3-卷积层涉及参数"><a href="#8-2-3-卷积层涉及参数" class="headerlink" title="8.2.3 卷积层涉及参数"></a>8.2.3 卷积层涉及参数</h6><p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_09-12-15.png" style="zoom:70%;"></p>
<ul>
<li>步长越小特征越多越丰富，计算也就越慢（图像一般就是1，文字等的就不好说了）</li>
<li>一般情况下卷积核最小为3*3</li>
<li>边缘填充：+pad X ，因为越往中间的点，对于计算时影响越大，因为有可能两次选择的区域有重合的部分，所以为了让边界点更多的利用，就进行边缘填充，填充用0，避免新添加的边缘对结果的影响</li>
</ul>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_09-18-27.png" style="zoom:80%;"></p>
<p>卷积参数共享</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_10-44-51.png" style="zoom:60%;"></p>
<p>就是说用同样的卷积核对输入的数据的每个区域进行计算，按理说每个区域不一样，但是那样参数就会太多了。</p>
<h6 id="8-2-4-池化层"><a href="#8-2-4-池化层" class="headerlink" title="8.2.4 池化层"></a>8.2.4 池化层</h6><p>池化层做压缩的，或者下采样，只能对h或者w进行缩减，但是特征图的个数c是不能变的</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_10-52-01.png" style="zoom:70%;"></p>
<p>做法就是指定一种方法，在原始得到的特征的基础上进行筛选，比如最大池化方法MAX POOLING</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_10-53-32.png" style="zoom:70%;"></p>
<ul>
<li>划分不同的区域，比如上图选择了2*2的区域</li>
<li>每个区域选最大的特征值点</li>
</ul>
<blockquote>
<p>为什么选最大的，因为在CNN中，越大的值代表结果越重要</p>
</blockquote>
<p>还有平均池化方法，也是划分区域然后计算平均值，现在很少见了，因为CNN只要最好的特征，不需要平均最好的特征。</p>
<p>举个CNN网路结构的例子</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_10-59-23.png" style="zoom:60%;"></p>
<ul>
<li>卷积层后面都有RELU激活函数，与传统CNN一样</li>
<li>两次卷积一次池化，基本都是卷积几次得到很大的特征图，然后池化进行压缩，再进行卷积</li>
<li>前面的卷积和池化只是做特征提取的，最后想得到结果还是靠全连接层FC，怎么算呢，其实在最后的FC之前会有一个拉长的操作，将前面得到的特征图（三维的）拉成一个一维的特征向量，比如前面的到32*32*10的特征图，特征向量大小九尾1*10240，然后这时候FC层大小就为10240*5，为什么是5，因为我要得到五个类别的分类结果，上图所示。</li>
<li>什么叫一层CNN呢，带参数计算的算1层，比如CONV层，激活层RELU等不算，池化层也不算，全连接FC层也算，他也带参数。</li>
<li>特征图变化</li>
</ul>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-16_11-07-15.png" style="zoom:85%;"></p>
<h4 id="9-VGG网络架构"><a href="#9-VGG网络架构" class="headerlink" title="9 VGG网络架构"></a>9 VGG网络架构</h4><ul>
<li><p>所有卷积都是3*3的，有16层卷积</p>
</li>
<li><p>当pooling时，损失的信息会在下次conv补回来</p>
</li>
<li><p>红框标出来是常用的</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_06-59-49.png" style="zoom:80%;"></p>
</li>
</ul>
<h4 id="10-残差网络Resnet"><a href="#10-残差网络Resnet" class="headerlink" title="10 残差网络Resnet"></a>10 残差网络Resnet</h4><p>深层网络遇到的问题，56层网络深度在测试时错误率比20层要高很多，因为就是无法保障上一层的训练数据比之前的更好，所以变深可能就出现偏差</p>
<img src="/2024/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Snipaste_2024-08-17_07-01-31.png" class="">
<p>解决方法：</p>
<img src="/2024/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Snipaste_2024-08-17_07-03-12.png" class="">
<blockquote>
<ul>
<li>x就是卷积中的某一层（20层之后）</li>
<li>左侧竖着的就是按照顺序进行卷积</li>
<li>右侧就是说加入我中间的有个卷积层不好，我直接跨过去了（同等映射到下下个层）</li>
</ul>
</blockquote>
<p>使用之后举个例子</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_07-28-34.png" style="zoom:70%;"></p>
<h4 id="11-感受野"><a href="#11-感受野" class="headerlink" title="11 感受野"></a>11 感受野</h4><p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_07-55-26.png" style="zoom:60%;"></p>
<p>什么是感受野呢，就是比如上图，堆叠了两个3*3卷积层，最后图的那个点那个可以看到第二个3*3的矩阵大小，第二个3*3的矩阵大小算下来可以看到第一个5*5的矩阵大小，所以感受野是5*5</p>
<p>假如说堆叠了3个3*3的卷积层，滑动步长为1，那么感受野就变成了7*7，这个和使用7*7的卷积核得到的结果是一样的，但是为啥还要堆叠三个小卷积核呢</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-00-29.png" style="zoom:60%;"></p>
<p>上图中说的不好的一个地方是，我猜测他的意思是，有C个卷积核（大），然后呢，每个大的概念的卷积核分为两种：</p>
<ul>
<li>一个7*7的卷积核</li>
<li>3个3*3的卷积核</li>
</ul>
<h4 id="12-RNN网络架构"><a href="#12-RNN网络架构" class="headerlink" title="12 RNN网络架构"></a>12 RNN网络架构</h4><p>递归神经网络（RNN，Recurrent Neural Networks），RNN处理序列数据时非常有效，它能够处理序列中的每一个元素，同时保持对序列中前元素的记忆。</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-08-45.png" style="zoom:60%;"></p>
<p>CNN：计算机视觉当中</p>
<p>RNN：自然语言处理当中（因为RNN可以考虑时间顺序，先后顺序）</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-15-48.png" style="zoom:70%;"></p>
<p>更直观的例子</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-21-21.png" style="zoom:80%;"></p>
<p>RNN的结构中包括输入层、隐藏层和输出层，但是特别之处在于隐藏层之间的节点是有连接的，即隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。这种结构使得RNN能够处理输入序列的上下文信息，并据此产生相应的输出序列。</p>
<p>因为RNN存在梯度消失以及梯度爆炸问题，所以出现了LSTM和CRU算法。同时对于RNN来说，最后一个结果会把前面的结果都考虑进来，但是前面的结果不一定有用，记得太多可能会产生误差，在LSTM中可以忘记一些特征。</p>
<h5 id="12-1-LSTM"><a href="#12-1-LSTM" class="headerlink" title="12.1 LSTM"></a>12.1 LSTM</h5><p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-28-20.png" style="zoom:60%;"></p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-29-09.png" style="zoom:60%;"></p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-29-31.png" style="zoom:60%;"></p>
<p>计算完的结果还需要前一轮的结果Ct-1进行比较计算</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-29-56.png" style="zoom:60%;"></p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-31-09.png" style="zoom:60%;"></p>
<p>计算方法</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-31-37.png" style="zoom:60%;"></p>
<p>输出</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-32-23.png" style="zoom:60%;"></p>
<p>整体结构</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-32-46.png" style="zoom:60%;"></p>
<h4 id="13-word2rec"><a href="#13-word2rec" class="headerlink" title="13 word2rec"></a>13 word2rec</h4><p>word2vec也叫word embeddings，中文名“词向量”，作用就是将自然语言中的字词转为计算机可以理解的稠密向量（Dense Vector）。在word2vec出现之前，自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder。</p>
<img src="/2024/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Snipaste_2024-08-17_08-43-38.png" class="">
<p>只要有了向量就可以使用不同的方法来计算相似度，比如</p>
<ul>
<li>欧氏距离</li>
<li>余弦相似度</li>
</ul>
<p>等等</p>
<p>通常，数据的维度越高，提供的信息也就越多，从而计算结果就越可靠（word2vec一般生成的向量在50-300维）</p>
<p>观察时可以使用热度图，更直观，看颜色就行</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-46-19.png" style="zoom:50%;"></p>
<p>词向量模型中输入和输出分别是啥</p>
<p>预测下一个词</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-48-36.png" style="zoom:50%;"></p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-50-01.png" style="zoom:50%;"></p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_08-51-39.png" style="zoom:50%;"></p>
<p>对于上图，就是说我有一个词库的大表，就是说输入之前需要到词库中找到它对应的向量（图上是4维），词库的表一开始是随机初始化的，随着训练的进行，会不断进行更新。</p>
<p>一切有逻辑的文本（不是乱拼乱凑的）都可以作为训练数据</p>
<h5 id="13-1-构建训练数据"><a href="#13-1-构建训练数据" class="headerlink" title="13.1 构建训练数据"></a>13.1 构建训练数据</h5><p>中文需要先分词再构建数据集，可以使用滑动窗口来不断构建数据集。</p>
<p>一般窗口是一个基数的。</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-17_09-17-10.png" style="zoom:80%;"></p>
<h5 id="13-2-CBOW以及Skip-gram模型"><a href="#13-2-CBOW以及Skip-gram模型" class="headerlink" title="13.2 CBOW以及Skip-gram模型"></a>13.2 CBOW以及Skip-gram模型</h5><p>CBOW就是输入是我的上下文，输出是中间的词</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-27_08-28-20.png" style="zoom:60%;"></p>
<p>Skip-gram就是根据当前的词去预测上下文</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-27_08-30-19.png" style="zoom:60%;"></p>
<p>上述两个模型简单演示，左侧为CBOW</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-27_08-30-50.png" style="zoom:60%;"></p>
<p>如果一个语料库稍微大一点，那么他可能的结果就会很多，最后一层相当一softmax，计算起来十分耗时，需要手段来解决。</p>
<p>初始方案：输入两个单词，看他们是不是前后对应的输入和输出，也就相当于一个二分类任务（原来是A预测B，现在是输入A和B，看B是A后面的概率）</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-27_08-36-29.png" style="zoom:60%;"></p>
<p>但是这样的话训练集构建出来的标签全为1，没法进行较好的训练</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-27_08-40-25.png" style="zoom:60%;"></p>
<p>改进：加入一些负样本（负采样模型），认为的创建一些语料库中上下文没有的词。正样本就是语料库中有上下文关系的。</p>
<img src="/2024/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Snipaste_2024-08-27_08-43-04.png" class="">
<h5 id="13-3-词向量训练过程"><a href="#13-3-词向量训练过程" class="headerlink" title="13.3 词向量训练过程"></a>13.3 词向量训练过程</h5><p>1 初始化词向量矩阵</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-27_08-51-13.png" style="zoom:60%;"></p>
<p>就是Embedding可以理解为input1，context可以理解为input2<br>然后找到上下文对应的矩阵</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-27_08-52-45.png" style="zoom:60%;"></p>
<p>2 通过神经网络反向传播来计算更新，此时不仅更新权重参数矩阵W，还要更新输入数据</p>
<p><img src="/2024/10/04/深度学习/Snipaste_2024-08-27_08-54-01.png" style="zoom:60%;"></p>
<h4 id="14-安装Keras"><a href="#14-安装Keras" class="headerlink" title="14 安装Keras"></a>14 安装Keras</h4><p>Keras更轻松，有很多API，TensorFlow用的人更多，PyTorch用的人也不少</p>
<p>安装keras</p>
<figure class="highlight powershell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install keras</span><br></pre></td></tr></tbody></table></figure>
<h4 id="15-训练自己的数据集"><a href="#15-训练自己的数据集" class="headerlink" title="15 训练自己的数据集"></a>15 训练自己的数据集</h4><p>对神经网络结果影响最大的就是学习率，会先指定base learning rate，基础学习率，要是结果不好，可以降低学习率，一般降低到十分之一</p>
<p>正则化越大，训练集和测试集得到的acc结果越接近</p>
<h4 id="END"><a href="#END" class="headerlink" title="END"></a>END</h4></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">expAddThree</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">http://example.com/2024/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">LXY's Notes</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post-share"><div class="social-share" data-image="/img/kanna.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-left" href="/2024/10/04/Redis%20%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%9F%BA%E7%A1%80/" title="Redis 第一章 基础"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Redis 第一章 基础</div></div></a><a class="next-post pull-right" href="/2024/10/04/%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%8EAI%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" title="区块链与AI读书笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">区块链与AI读书笔记</div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/img/kanna.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info-name">expAddThree</div><div class="author-info-description">5L2g5aW977yBCg==</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">55</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/expAdd3"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:lixybupt@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">An announcement means no announcement! (^_^)</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-text">深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="toc-text">1 机器学习的流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-text">2 深度学习的应用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%BB%BB%E5%8A%A1"><span class="toc-text">3 计算机视觉任务</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="toc-text">4 K近邻算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-text">5 神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#5-1-%E5%BE%97%E5%88%86%E5%87%BD%E6%95%B0%EF%BC%88%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0%EF%BC%89"><span class="toc-text">5.1 得分函数（线性函数）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">5.2 损失函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-3-Softmax%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-text">5.3 Softmax分类器</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">6 反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#6-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">6.1 梯度下降</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">7 神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#7-1-%E7%A5%9E%E7%BB%8F%E5%85%83%E4%B8%AA%E6%95%B0%E5%AF%B9%E7%BB%93%E6%9E%9C%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">7.1 神经元个数对结果的影响</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#7-2-%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">7.2  正则化与激活函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#7-3-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">7.3 数据预处理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#7-4-%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">7.4 参数初始化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#7-5-DROP-OUT"><span class="toc-text">7.5 DROP-OUT</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">8 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#8-1-%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E4%B8%8E%E4%BC%A0%E7%BB%9F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">8.1 卷积网络与传统网络的区别</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#8-2-CNN%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="toc-text">8.2 CNN整体架构</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#8-2-1-%E5%8D%B7%E7%A7%AF"><span class="toc-text">8.2.1 卷积</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#8-2-2-%E5%9B%BE%E5%83%8F%E9%A2%9C%E8%89%B2%E9%80%9A%E9%81%93"><span class="toc-text">8.2.2 图像颜色通道</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#8-2-3-%E5%8D%B7%E7%A7%AF%E5%B1%82%E6%B6%89%E5%8F%8A%E5%8F%82%E6%95%B0"><span class="toc-text">8.2.3 卷积层涉及参数</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#8-2-4-%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-text">8.2.4 池化层</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-VGG%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-text">9 VGG网络架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9CResnet"><span class="toc-text">10 残差网络Resnet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-text">11 感受野</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-RNN%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-text">12 RNN网络架构</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#12-1-LSTM"><span class="toc-text">12.1 LSTM</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-word2rec"><span class="toc-text">13 word2rec</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#13-1-%E6%9E%84%E5%BB%BA%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-text">13.1 构建训练数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#13-2-CBOW%E4%BB%A5%E5%8F%8ASkip-gram%E6%A8%A1%E5%9E%8B"><span class="toc-text">13.2 CBOW以及Skip-gram模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#13-3-%E8%AF%8D%E5%90%91%E9%87%8F%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text">13.3 词向量训练过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-%E5%AE%89%E8%A3%85Keras"><span class="toc-text">14 安装Keras</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#15-%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">15 训练自己的数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#END"><span class="toc-text">END</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/09/Leetcode-Day18/" title="Leetcode-Day18">Leetcode-Day18</a><time datetime="2024-12-09T02:06:53.000Z" title="发表于 2024-12-09 10:06:53">2024-12-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/11/Dive%20Into%20Deep%20Learning/" title="Dive Into Deep Learning">Dive Into Deep Learning</a><time datetime="2024-11-11T06:48:59.000Z" title="发表于 2024-11-11 14:48:59">2024-11-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/10/Leetcode-Day17/" title="Leetcode-Day17">Leetcode-Day17</a><time datetime="2024-11-10T04:17:14.000Z" title="发表于 2024-11-10 12:17:14">2024-11-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/09/Leetcode-Day16/" title="Leetcode-Day16">Leetcode-Day16</a><time datetime="2024-11-09T03:54:41.000Z" title="发表于 2024-11-09 11:54:41">2024-11-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/06/Java%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%91%9E%E5%90%89%E5%A4%96%E5%8D%96/" title="Java项目实战-瑞吉外卖">Java项目实战-瑞吉外卖</a><time datetime="2024-11-06T11:41:33.000Z" title="发表于 2024-11-06 19:41:33">2024-11-06</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">©2019 - 2024 By expAddThree</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>